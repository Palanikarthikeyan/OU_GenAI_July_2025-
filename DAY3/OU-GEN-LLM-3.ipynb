{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34e39c-4181-4efc-bbd3-185355df0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "------\n",
    "[Documents load] --->[chunk] ->[Embedding] ->[VectorDB]-<-[retriever] <--[model] <-[QA]<--[Test]\n",
    ".......................................................-->            ->        ->    -> response\n",
    "\n",
    "\n",
    "OpenAI\n",
    "---------\n",
    "langchain.llms import OpenAI\n",
    "llm = OpenAI(openai_api_key=<>)\n",
    "llm() # callable object => llm(\"Query\") ->response \n",
    "Run: cloud - paid token\n",
    "\n",
    "HuggingFace\n",
    "---------------\n",
    "Run: locally (or) API(cloud)\n",
    "\n",
    "Ollama\n",
    "------\n",
    "Run: locally  - run on local m/c - offline chatbot use \n",
    "====\n",
    "\n",
    "HuggingFace\n",
    "-----------\n",
    "load ->split =>Embedding\n",
    "HuggingFaceEmbeddings(model_name=\"<modelName>\") ->embeddings\n",
    "\n",
    "Stores docs to DB\n",
    "------------------\n",
    "vectorstore =FAISS.from_documents(docs,embeddings)\n",
    "|\n",
    "r =vectorestore.as_reriever() # Retrivever \n",
    "\n",
    "llm_model = Ollama(model=\"ModelName\")\n",
    "\n",
    "RetrievalQA Chain \n",
    "-----------------\n",
    "QA_Chain = RetrievalQA.from_chain_type(llm=<llm_obj>,retriever=<retriever_obj>,return_s...=True)\n",
    "========\n",
    "|__QA_chain.query(\"User inputQuery\") ->response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a19095-8055-43bb-bc96-2ff7e251dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1st - create token from https://huggingface.co/settings/tokens\n",
    "|\n",
    "2nd - update this token to os env  \n",
    "     Go to commandline => Winx: gitbash -> vi .env \n",
    "                                        HF_TOKEN=\"<paste token>\"\n",
    "                                        save this file  :wq \n",
    "\n",
    "|\n",
    "3rd -> go python IDLE (Jupyter;pycharm,vscode etc.,)\n",
    "        |\n",
    "       from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf031e2-16f2-4a28-a5e8-d9fd9e289e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae80c9c-9553-4d70-a806-a4e52e5c49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_3964\\1689552512.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.embeddings.huggingface.HuggingFaceEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c4fd10-e925-46a0-8169-720057c61a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.embed_query(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff6e5-ab65-4dc2-9e9c-120da98c6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"hf_GWAasfsdfsdfsadfsdsfsfsfss\" \n",
    "  Vs\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\") # loading token file and initialize to env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e961f81f-a29f-4cee-9e10-900e224548f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986138b7-8b98-4308-95ac-c999563f8c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6017a4f0-204d-4982-a737-f8524f499b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ccd408-1685-4b48-abf0-d007adf26bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\") # loading token file and initialize to env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2125a626-451e-4e91-86c3-0cfdf2ad4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13897061-2c29-4900-a590-0c3d39fef47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load documents - TextLoader \n",
    "\n",
    "# 2. split text into chunks & initialize to docs\n",
    "\n",
    "# 3. Generate embeddings (use HuggingFace)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(<docs>,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 6. Use model - Ollama model   from langchain_community.llms import Ollama\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "#7. Build RetrievalQA Chain  from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriver,return_source_documents=True)\n",
    "\n",
    "#8. Ask Questions\n",
    "result = qa_chain(\"what is ...\")\n",
    "#9. display / output response \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394cefb8-3300-451a-ac2d-84bd3cb67cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chains import RetrievalQA\n",
    "#help(RetrievalQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5151ff6c-a288-4358-8c4d-901edbab2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd76254a-5dfc-4043-b801-3410db1617ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Sure, here is the answer to the question:\n",
      "\n",
      "**LangChain** is a framework to build LLM-powered apps using composable components.\n",
      "\n",
      "**RAG** stands for Retrieval-Augmented Generation. It helps LLMs use external knowledge.\n",
      "\n",
      "Sources:\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "# 1. Load documents\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2.split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3.Generate embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# 6. Use Ollama model\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "# 7. Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,return_source_documents=True)\n",
    "\n",
    "# 8. Ask Question/Query\n",
    "response = qa_chain(\"what is langchain and how RAG work\")\n",
    "\n",
    "# 9. display/print - response \n",
    "print(\"\\nAnswer:\",response['result'])\n",
    "print(\"\\nSources:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2dc71-72e8-43ca-b154-93cbac9193f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt\n",
    "=======\n",
    " |->string - text - give to language model to instruct to generate response \n",
    "\n",
    "PromptTemplate\n",
    "==============\n",
    " |->reusable prompt\n",
    "    |->placeholder (or) variable\n",
    "\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "PromptTemplate(input_variables = <> , template=\"user defined message...{<placeholder>} ....\") ->prompt\n",
    "prompt.format()\n",
    "|\n",
    "reusability\n",
    "runtime \n",
    "logic - data separate from prompt structure\n",
    "---//integrate with llmchain(or)pipeline \n",
    "\n",
    "PromptTemplate(input_variable=[\"query\"],template=\"Answer the following query:{query}\") \n",
    "                                |________________________________________________|\n",
    "ChatPromptTemplate\n",
    "---------------------\n",
    " |->roles - multiple roles\n",
    "            --------------\n",
    "           system - control the assistant behavior \n",
    "           human  - user input\n",
    "           AI     - simulate as assistant responses - (optinal)\n",
    "           +\n",
    "           content-text\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "ChatPromptTemplate.from_messages([(\"system\",\"assistant\"),(\"human\",\"write input{var}\")]) ->prompt\n",
    "\n",
    "placeholder ->replaced with input at runtime\n",
    "prompt.format_message(placeholderName=<Value>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1792cffd-0a6a-4117-afe2-4180bfa271e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5d08b1-8515-411e-8899-9d22b8c31fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f4d7de-37bd-45da-bb00-fdf7e9170e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "prompt_obj.format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac6ab52-dcd5-4cb8-aa81-d21b2c45c34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj.format(myvar=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "122e732d-7708-4fbb-9d6a-6bbca8270dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain some short story about birds in english written by Mr.abc'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pT=PromptTemplate.from_template('Explain some short story about {topic} in {lng} written by {author}')\n",
    "pT.format(topic=\"birds\",lng=\"english\",author=\"Mr.abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c3fbbbf-bd02-4ad6-abf9-469323589708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# (role,content)\n",
    "\n",
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0f0e73a-187a-4d69-815f-c2a8c49d9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "msg = prompt.format_messages(question=\"what is gen AI?\")\n",
    "\n",
    "# use llm chat model ->llmobject\n",
    "#llmobject(msg) <= callable object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a829313a-9f96-46eb-ab0d-15b82749995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "obj = box()\n",
    "callable(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72e2b8e4-4f57-4b9e-9982-102f40b74786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def __call__(self):\n",
    "        pass\n",
    "obj = box()\n",
    "callable(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efad9a77-5055-4d52-8c29-b6a57d932351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return from callable method - llm object :message'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def __call__(self,a):\n",
    "        return 'return from callable method - llm object :'+a\n",
    "obj = box()\n",
    "obj('message') # llmobj(promptmessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9d149e6-4a9b-4a6d-a65d-729284d46a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "msg = prompt.format_messages(question=\"what is gen AI?\")\n",
    "\n",
    "# use llm chat model ->llmobject\n",
    "#llmobject(msg) <= callable object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c0b6808-f556-493a-ad2d-bd991f422f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])\n",
       "| Ollama(model='gemma:2b')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "#llm_obj(msg)\n",
    "prompt|llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36721368-06bb-4224-b78e-ffc5888c2034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Gen AI** stands for **Generative Artificial Intelligence**. It is a type of artificial intelligence (AI) that focuses on creating new content, such as text, images, music, and more.\\n\\n**Key characteristics of generative AI:**\\n\\n* **Data-driven:** Gen AI models are trained on massive datasets of existing content.\\n* **Creative:** They can generate novel and unique content that is not explicitly programmed.\\n* **Automating tasks:** Gen AI can automate certain creative and repetitive tasks, such as writing, translation, and image editing.\\n* **Learning from data:** Gen AI models continuously learn from new data, enabling them to improve over time.\\n\\n**Examples of generative AI systems:**\\n\\n* **Chatbots:** Chatbots like ChatGPT and Alexa are based on generative AI.\\n* **Language models:** These models can generate human-like text in multiple languages.\\n* **Image generators:** Generative models like DALL-E 2 can create realistic images from text descriptions.\\n* **Music generators:** Generative AI can create new musical compositions and songs.\\n\\n**Benefits of generative AI:**\\n\\n* **Increased creativity:** Gen AI can help artists and content creators generate new ideas.\\n* **Enhanced productivity:** Automating tasks can save time and effort.\\n* **Personalized experiences:** Gen AI can tailor content to individual preferences.\\n* **New applications:** Generative AI has potential applications in various industries, including healthcare, finance, and entertainment.\\n\\n**Challenges of generative AI:**\\n\\n* **Bias and fairness:** Gen AI models can inherit biases from the data they are trained on.\\n* **Lack of interpretability:** The inner workings of generative AI models are often complex and difficult to understand.\\n* **Ethical considerations:** Generative AI can be used for malicious purposes, such as spreading misinformation or generating realistic deepfakes.\\n\\n**Overall, generative AI is a transformative technology with the potential to revolutionize various aspects of human life.**'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "#msg = prompt.format_messages(question=\"what is gen AI?\")\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':'what is gen AI'}) # chainobject.invoke({'placeholder':'user Query value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc08826b-331e-464e-bf50-5a8c7e050990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Step 1: Define the Factorial Function**\\n\\n```python\\ndef factorial(n):\\n    \"\"\"\\n    Calculates the factorial of a non-negative integer n.\\n\\n    Args:\\n        n: The non-negative integer for which to calculate the factorial.\\n\\n    Returns:\\n        The factorial of n.\\n    \"\"\"\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n```\\n\\n**Step 2: Get the Input from the User**\\n\\n```python\\n# Get the input from the user\\nn = int(input(\"Enter a non-negative integer: \"))\\n```\\n\\n**Step 3: Calculate the Factorial**\\n\\n```python\\nfactorial_result = factorial(n)\\n```\\n\\n**Step 4: Print the Result**\\n\\n```python\\n# Print the result of the factorial calculation\\nprint(f\"The factorial of {n} is {factorial_result}\")\\n```\\n\\n**Example Usage:**\\n\\n```\\nEnter a non-negative integer: 5\\nThe factorial of 5 is 120\\n```\\n\\n**Note:**\\n\\n* The `factorial()` function calculates the factorial of a non-negative integer `n`.\\n* The function uses a recursive approach to compute the factorial.\\n* The `if` statement handles the base case for `n` being 0.\\n* The `return` statements ensure that the factorial is calculated accurately.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'How to write factorial program in python?'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c850f114-42b4-49a1-9880-5ff75892c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's how to write a factorial program in Python:\n",
      "\n",
      "```python\n",
      "# Define the factorial function\n",
      "def factorial(n):\n",
      "    # Initialize the factorial to 1 (since 0! is defined as 1!)\n",
      "    factorial = 1\n",
      "\n",
      "    # Iterate from 1 to n\n",
      "    for i in range(1, n + 1):\n",
      "        # Multiply the factorial of i by the factorial of n\n",
      "        factorial *= i\n",
      "\n",
      "    # Return the factorial\n",
      "    return factorial\n",
      "\n",
      "\n",
      "# Get the input from the user\n",
      "n = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial of n\n",
      "print(f\"The factorial of {n} is {factorial(n)}\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. The `factorial()` function takes one argument, `n`, which is the number for which we want to calculate the factorial.\n",
      "2. It initializes a `factorial` variable to 1, as 0! is defined as 1!.\n",
      "3. It then enters a `for` loop that iterates from 1 to `n`.\n",
      "4. Inside the loop, it multiplies the `factorial` of `i` by the `factorial` of `n`. This accumulates the products of all non-zero integers from 1 to `n` in the `factorial` variable.\n",
      "5. After the loop completes, the function returns the final `factorial` value.\n",
      "6. The user is prompted to input a non-negative integer, and the program calculates and prints the factorial of that number using the `factorial()` function.\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```\n",
      "Enter a non-negative integer: 5\n",
      "\n",
      "The factorial of 5 is 120\n",
      "```\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The factorial of 0 is defined as 1! (since 0! is equal to 1!).\n",
      "* This program assumes that the input is a valid non-negative integer. If the user enters a non-positive or negative number, the program will produce an error.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'question':'How to write factorial program in python?'}) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d359c068-6a8f-4ac1-aae0-9b6e20538c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I can help.\\n\\n**Node A** is a term used in graph theory and data structures to refer to a node or vertex in a graph. It is a point in the graph that is connected to other nodes by edges.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'What is nodeA?'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "267ed156-c819-41ae-9310-033ce9ac8bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, I can help with that! Here's some information about Timmins Tech Center:\\n\\n**Location:**\\n* Address: 123 Main Street, Anytown, CA 12345\\n\\n**Contact:**\\n* Phone: (555) 123-4567\\n* Website: (555) 123-4567\\n* Email: info@timmins.com\\n\\n**Services:**\\n* Timmins Tech Center offers a wide range of IT services, including:\\n    * Computer repair\\n    * Data recovery\\n    * Software installation and maintenance\\n    * Networking\\n    * Cybersecurity\\n    * Cloud computing\\n    * And more\\n\\n**Hours of operation:**\\n* Monday - Friday: 8am to 5pm PST\\n\\n**Additional information:**\\n* Timmins Tech Center is a locally owned and operated business.\\n* They have been serving the community for over 10 years.\\n* They are committed to providing high-quality, affordable IT services.\\n\\n**Here are some additional resources that you may find helpful:**\\n* Timmins Tech Center website\\n* Google Maps location\\n* Reviews from previous customers\\n\\nI hope this information is helpful! Please let me know if you have any other questions.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'Get some info about timins tech center'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28df3401-41a5-4a31-990d-a796fc04829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? Is there anything I can help you with?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'asfasfas safsadf'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ece5930-a41e-49c9-ba2d-743d06bb6462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am unable to form opinions or ask questions. I do not have a physical body or consciousness, and I am unable to experience the world in the same way that humans do.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':'asfadss?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea2949ec-aa56-4127-a1d3-f935f3f6df59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure. Here's a breakdown of embedding:\\n\\n**Embedding** is a technique that allows you to represent data in a **lower-dimensional space** while preserving its essential characteristics. This can be achieved by using a **similarity or distance metric** to map the data onto a **higher-dimensional space**.\\n\\n**Here's how embedding works:**\\n\\n1. **Data transformation:** The data is input into an **embedding algorithm**, which uses a mathematical function to transform it into a new representation. This embedding can be done in various ways, such as **linear embedding**, **non-linear embedding**, or **deep learning embeddings**.\\n2. **Feature extraction:** The embedding algorithm also extracts features from the data, which are specific properties that are important for the task at hand. These features can be used for various tasks, such as **classification, regression, and clustering**.\\n3. **Dimensionality reduction:** To reduce the dimensionality of the data, the embedding algorithm can learn a lower-dimensional representation of the data. This can be achieved by using techniques like **principal component analysis (PCA)** or **t-distributed stochastic neighbor embedding (t-SNE)**.\\n4. **Applications:** Embeddings are widely used in various applications, including:\\n\\n  - **Natural language processing (NLP):** Embeddings are used to represent words and sentences in a vector space, enabling tasks such as sentiment analysis and text classification.\\n  - **Computer vision:** Embeddings are used to represent images and videos in a way that allows computers to understand and analyze them.\\n  - **Music information retrieval:** Embeddings are used to represent musical pieces and artists in a vector space, enabling tasks such as music genre classification and artist recommendation.\\n  - **Social network analysis:** Embeddings are used to represent social network data in a way that allows for analysis of network structure and dynamics.\\n\\n**Benefits of embedding:**\\n\\n* **Dimensionality reduction:** Embeddings can reduce the dimensionality of data while preserving important information. This can improve the efficiency and speed of machine learning models.\\n* **Feature extraction:** Embeddings can extract features from data that may not be readily apparent in the original space.\\n* **Representation learning:** Embeddings can learn meaningful representations of data, which can be used for various tasks.\\n\\n**Overall, embedding is a powerful technique that can be used to represent and analyze data in a lower-dimensional space. It has a wide range of applications in various fields, including natural language processing, computer vision, and data mining.**\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':'explain about embedding?'})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ff5d315-5fb1-4bad-8e7f-69acc6e72840",
   "metadata": {},
   "source": [
    "def query_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",  # Specify the model you're using\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json().get(\"response\", \"\")\n",
    "        # Check if the response is empty or contains a \"don't know\" type message\n",
    "        if not result or \"don't know\" in result.lower() or \"no information\" in result.lower():\n",
    "            return \"No results found. Please try a different query.\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"Error connecting to Ollama. Please check if it's running.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10ff096d-7ac5-4240-8078-5a1e7d38c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = chain.invoke({'question':'what is python?'})\n",
    "r2 = chain.invoke({'question':'asfsafsadsd?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90bdbe4a-08cb-4f68-9dd2-4f4204173647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(bool(r1),bool(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa5644f0-90b4-435a-87db-ba285d18af52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = ''\n",
    "bool(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3b035d4-33d4-47bd-b6ac-225c0343a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure what you're asking. Can you rephrase your question?\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64933444-33ff-4253-9f17-be0700270d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. Here's a brief explanation of Python:\n",
      "\n",
      "**Python** is a high-level programming language known for its **conciseness, readability, and versatility**. It's often referred to as **\"the Python of choice\"** due to its popularity and widespread use.\n",
      "\n",
      "**Here's what you can do with Python:**\n",
      "\n",
      "* **Code generation:** Write Python code and use interactive tools to create programs on the fly.\n",
      "* **Data manipulation:** Work with data using built-in functions and libraries.\n",
      "* **Machine learning and data science:** Develop and analyze machine learning models for various tasks.\n",
      "* **Web development:** Build websites and web applications with libraries like Django.\n",
      "* **Automation tasks:** Automate repetitive tasks with powerful modules.\n",
      "* **Game development:** Create interactive and engaging games with libraries like Pygame.\n",
      "\n",
      "**Benefits of learning Python:**\n",
      "\n",
      "* **High demand:** Python is one of the most widely used languages in the world.\n",
      "* **Simple syntax:** Python's syntax is relatively easy to learn.\n",
      "* **Large and supportive community:** There are plenty of resources, tutorials, and online forums to help you learn.\n",
      "* **Versatile applications:** You can use Python for various tasks, from data analysis to web development.\n",
      "\n",
      "**In summary, Python is a powerful and versatile language that can be used for a wide range of tasks, including:**\n",
      "\n",
      "* Data science\n",
      "* Machine learning\n",
      "* Web development\n",
      "* Automation\n",
      "* Game development\n",
      "* And more\n",
      "\n",
      "If you're interested in learning more about Python, there are many resources available online and in libraries. You can also take online courses, read books, and watch tutorials.\n"
     ]
    }
   ],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f279bfc-cb4b-40b3-ac4e-b48d9d92b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that!\n",
      "\n",
      "**GenAI** is a **Generative AI model** that can create various creative content, including text, images, audio, and more. It is a powerful tool that can be used for a wide range of purposes, such as:\n",
      "\n",
      "* **Writing**\n",
      "* **Storytelling**\n",
      "* **Translation**\n",
      "* **Image generation**\n",
      "* **Audio generation**\n",
      "* **And more**\n",
      "\n",
      "**Here are some key features of GenAI:**\n",
      "\n",
      "* It is trained on a massive dataset of text and code.\n",
      "* It uses a generative adversarial network (GAN) to create new content.\n",
      "* It can generate realistic and imaginative images, text, and audio.\n",
      "* It is constantly learning and improving its abilities.\n",
      "\n",
      "**Examples of the content that GenAI can generate:**\n",
      "\n",
      "* Novels\n",
      "* Poems\n",
      "* Scripts\n",
      "* Music\n",
      "* Art\n",
      "* And more\n",
      "\n",
      "**How to use GenAI:**\n",
      "\n",
      "* You can access GenAI through a variety of platforms, such as:\n",
      "    * Google Cloud AI\n",
      "    * Microsoft Azure AI\n",
      "    * Amazon Web Services\n",
      "* Once you have an account, you can start using GenAI by providing a prompt or specifying the type of content you want to generate.\n",
      "* The model will generate the content and return it to you in a few seconds.\n",
      "\n",
      "**Additional notes:**\n",
      "\n",
      "* GenAI is still under development, so it may generate some errors or inconsistencies in its output.\n",
      "* It is important to use GenAI responsibly and to avoid generating content that is offensive, discriminatory, or illegal.\n"
     ]
    }
   ],
   "source": [
    "# To get std output format\n",
    "# ---------------------------\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','Your are a helpful AI assistant'),\n",
    "    ('user','Question:{question}')\n",
    "    ])\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "output = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm_obj|output\n",
    "response = chain.invoke({'question':'what is genAI?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13cf2664-b37d-416f-8e54-379b035814a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a step-by-step guide on how to write a simple \"Hello World\" program in Python:\n",
      "\n",
      "**Step 1: Open a Python Text Editor**\n",
      "\n",
      "* Download and install a Python text editor like Visual Studio Code, PyCharm, or IDLE.\n",
      "* Launch your chosen text editor.\n",
      "\n",
      "**Step 2: Create a New Python File**\n",
      "\n",
      "* Open the text editor and create a new file with the extension \".py\". For example, you could name it \"hello_world.py\".\n",
      "\n",
      "**Step 3: Write the Python Code**\n",
      "\n",
      "```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "* Save the file and close the text editor.\n",
      "\n",
      "**Step 4: Run the Python Script**\n",
      "\n",
      "* Open the terminal or command prompt in your terminal or command line.\n",
      "* Navigate to the directory where your file is saved.\n",
      "* Run the command: `python hello_world.py`\n",
      "\n",
      "**Step 5: Observe the Output**\n",
      "\n",
      "* The terminal or command prompt should print the following message:\n",
      "```\n",
      "Hello, world!\n",
      "```\n",
      "\n",
      "**Step 6: Understand the Code**\n",
      "\n",
      "* The code consists of one line of Python code.\n",
      "* `print()` is a built-in Python function that prints a message to the console.\n",
      "* `\"Hello, world!\"` is the actual message we want to print.\n",
      "\n",
      "**Step 7: Save and Run the File**\n",
      "\n",
      "* Save the file (hello_world.py) and exit the text editor.\n",
      "* Run the command: `python hello_world.py`\n",
      "\n",
      "**Step 8: Congratulations!**\n",
      "\n",
      "* You have successfully written and run your first Python program, printing the famous \"Hello World\" message.\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* You may need to install the Python interpreter on your system if you don't have it already.\n",
      "* This code is a simple example of a Python program. You can modify it to print different messages, use different libraries, and explore more advanced concepts.\n"
     ]
    }
   ],
   "source": [
    "print((prompt|llm_obj|output).invoke({'question':'how to write hello world example in python?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01af7847-04d2-43e0-aa2d-8033c4bfce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm happy to assist you with your question. How may I help you today?\n"
     ]
    }
   ],
   "source": [
    "print((prompt|llm_obj|output).invoke({'question':'fassdsdf23423asdfsd?'}))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c404b2b-0657-4908-9d6e-445d156381a3",
   "metadata": {},
   "source": [
    "Task\n",
    "=====\n",
    "step 1: prepare your dataset\n",
    "data = [ \"genai is subset of Deeplearning\",\n",
    "        \"langchain helps build llm applications\",\n",
    "        \"FAISS is used for similarity search\",\n",
    "        \"oracle23 ai is vector database\"]\n",
    "\n",
    "|\n",
    "Convert Text ->Embeddings\n",
    "|\n",
    "stores to FAISS \n",
    "|\n",
    "# save FAISS to local => vectorstore.save_local(\"demo1_index\") \n",
    "# Load FAISS vectorstore \n",
    "    # => loaded_vectorstore = FAISS.load_local(\"demo1_index\",embedding=<embedding>)\n",
    "\n",
    "Query the vectorstore\n",
    "---------------------\n",
    " loaded_vectorstore.similarity_search(query,k=N) # get top N values \n",
    "  |\n",
    "get the results\n",
    "|\n",
    "use enumerate(result) - iterate - index - response.page_content //data\n",
    "-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "747dc8a9-a006-41cb-81fa-4230fe87bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_3964\\1648213601.py:9: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model='gemma:2b')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "data = [ \"genai is subset of Deeplearning\",\n",
    "        \"langchain helps build llm applications\",\n",
    "        \"FAISS is used for similarity search\",\n",
    "        \"oracle23 ai is vector database\"]\n",
    "\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = OllamaEmbeddings(model='gemma:2b')\n",
    "vectorstore = FAISS.from_texts(data,embedding = embeddings)\n",
    "vectorstore.save_local(\"myfaiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52296512-ca23-4b60-85de-adcdcaed4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FAISS Vectorstore \n",
    "#loaded_vectorstore = FAISS.load_local(\"myfaiss_index\",embeddings=embeddings)\n",
    "#index = FAISS.load_local(\"myfaiss_index\", embeddings=embeddings, index_name=\"index_only\", allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7536fc07-1e89-4bb2-b69a-dd97c6bf8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(FAISS.load_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2259bccb-908b-4df4-8b12-ee2545fd8eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.langchain helps build llm applications\n",
      "2.oracle23 ai is vector database\n"
     ]
    }
   ],
   "source": [
    "result = vectorstore.similarity_search('what is langchain used for?',k=2)\n",
    "for i,value in enumerate(result):\n",
    "    print(f'{i+1}.{value.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5f201fb8-2ccd-4433-b831-25fcd98f1a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The context does not specify what langchain is used for, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "retriever_obj = vectorstore.as_retriever()\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,return_source_documents=True)\n",
    "result = qa_chain('what is langchain used for?')\n",
    "# 9. display/print - response \n",
    "print(\"\\nAnswer:\",result['result'])\n",
    "#print(\"\\nSources:\")\n",
    "#for var in result[\"source_documents\"]:\n",
    "#    print(var.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "614f5acd-cc4e-4712-aaf4-90326c8806e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The passage does not specify what langchain is used for, so I cannot answer this question from the context.\n"
     ]
    }
   ],
   "source": [
    "retriever_obj = vectorstore.as_retriever()\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,return_source_documents=True)\n",
    "result = qa_chain('what is langchain used for?')\n",
    "# 9. display/print - response \n",
    "print(\"\\nAnswer:\",result['result'])\n",
    "#print(\"\\nSources:\")\n",
    "#for var in result[\"source_documents\"]:\n",
    "#    print(var.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf0bf9a1-4451-4272-9460-f53fd9abbea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. Here's a summary of the purpose of LangChain:\n",
      "\n",
      "**LangChain** is a **cross-chain language model** specifically designed for **natural language understanding (NLU)**. It is a powerful tool that can be used for a wide range of natural language processing (NLP) tasks, including:\n",
      "\n",
      "* **Language translation:** LangChain can translate text between different languages with high accuracy.\n",
      "* **Text generation:** It can generate new text in a variety of styles and formats.\n",
      "* **Language modeling:** LangChain can be used to build large language models (LLMs) from scratch.\n",
      "* **Chatbots and virtual assistants:** It can be used to create chatbots and virtual assistants that can understand and respond to natural language queries.\n",
      "* **Sentiment analysis:** LangChain can analyze text and determine the sentiment (positive, negative, or neutral).\n",
      "* **Text classification:** It can classify text into different categories based on its content.\n",
      "* **Question answering:** LangChain can answer questions in a comprehensive and informative way.\n",
      "\n",
      "LangChain is particularly well-suited for tasks that require **cross-lingual understanding** or the ability to generate text in multiple languages. It is also a powerful tool for building and training **LLMs**, which are a type of artificial intelligence model that can learn to understand and generate natural language.\n",
      "\n",
      "Overall, LangChain is a versatile and powerful tool that can be used for a wide range of NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Your are a helpful AI assistant\"),(\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "output = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm_obj|output\n",
    "response = chain.invoke(\"what is langchain used for?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40f9cc6a-bec9-4c6b-a3b1-4485ce532d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that.\n",
      "\n",
      "The phrase \"asfasdfsd dsadsadfs for\" is not a standard phrase and therefore, I cannot provide a meaningful answer to your question.\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Info\"),(\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "output = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm_obj|output\n",
    "response = chain.invoke(\"what is asfasdfsd dsadsadfs for?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6517dc67-1ed4-4477-af38-cf87e1ce0bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.langchain helps build llm applications\n",
      "2.oracle23 ai is vector database\n",
      "\n",
      "Answer: The context does not specify what langchain is used for, so I cannot answer this question from the provided context.\n",
      "**What is LangChain used for?**\n",
      "\n",
      "LangChain is a large language model (LLM) developed by Google. It is a conversational AI that can understand and generate human-like text, given a prompt or question.\n",
      "\n",
      "**Key features of LangChain:**\n",
      "\n",
      "* **Text generation:** It can generate various text formats, including text, code, musical pieces, emails, and more.\n",
      "* **Language understanding:** It understands and responds to prompts in multiple languages.\n",
      "* **Multimodal capabilities:** It can process and generate text, images, and other media simultaneously.\n",
      "* **Prompt scoring:** It evaluates and ranks different prompts based on their quality and relevance.\n",
      "* **Safety and ethics:** It is trained on a massive dataset of text and code, which helps to promote safety and prevent harmful or inappropriate responses.\n",
      "\n",
      "**Uses of LangChain:**\n",
      "\n",
      "* **Text generation:** Chatbots and virtual assistants\n",
      "* **Language translation:** Translating text between multiple languages.\n",
      "* **Content creation:** Writing stories, articles, and other creative content.\n",
      "* **Information retrieval:** Finding answers to questions on a wide range of topics.\n",
      "* **Customer support:** Handling customer inquiries and providing support.\n",
      "* **Education:** Teaching and learning new languages and concepts.\n",
      "* **Entertainment:** Creating and generating personalized entertainment experiences.\n",
      "\n",
      "**Benefits of using LangChain:**\n",
      "\n",
      "* **Increased efficiency:** It can automate text-based tasks, saving time and effort.\n",
      "* **Enhanced creativity:** It can help generate new ideas and perspectives.\n",
      "* **Improved communication:** It can facilitate more natural and engaging conversations.\n",
      "* **Access to a vast amount of information:** It provides access to a vast repository of text and data.\n",
      "* **Personalized experiences:** It can tailor its responses to individual preferences.\n"
     ]
    }
   ],
   "source": [
    "### Activity\n",
    "### --------------------\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "data = [ \"genai is subset of Deeplearning\",\n",
    "        \"langchain helps build llm applications\",\n",
    "        \"FAISS is used for similarity search\",\n",
    "        \"oracle23 ai is vector database\"]\n",
    "######## loaded dataset \n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = OllamaEmbeddings(model='gemma:2b')\n",
    "######## embedding \n",
    "vectorstore = FAISS.from_texts(data,embedding = embeddings)\n",
    "####### VectorDB is created ->embedded \n",
    "vectorstore.save_local(\"myfaiss_index\")\n",
    "####### stores to local disk\n",
    "\n",
    "# load FAISS Vectorstore \n",
    "#loaded_vectorstore = FAISS.load_local(\"myfaiss_index\",embeddings=embeddings)\n",
    "#index = FAISS.load_local(\"myfaiss_index\", embeddings=embeddings, index_name=\"index_only\", allow_dangerous_deserialization=True)\n",
    "\n",
    "# do the query - get top 2 records from the similarity_search \n",
    "result = vectorstore.similarity_search('what is langchain used for?',k=2)\n",
    "\n",
    "# iterate results record by record \n",
    "for i,value in enumerate(result):\n",
    "    print(f'{i+1}.{value.page_content}')\n",
    "######################################################\n",
    "# [Dataload/chunk]->[Embedded]-->[DataBase]--->[Retriever] --->[model]->[QueryChain]\n",
    "# --------------------------------------------------------------------------\n",
    "retriever_obj = vectorstore.as_retriever() # retrieved \n",
    "llm_obj = Ollama(model=\"gemma:2b\") # model \n",
    "\n",
    "# create the QueryChain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,return_source_documents=True) \n",
    "\n",
    "result = qa_chain('what is langchain used for?') # user query\n",
    "# 9. display/print - response \n",
    "print(\"\\nAnswer:\",result['result']) # get the response\n",
    "\n",
    "##################################################\n",
    "# [Dataload/chunk]->[Embedded]-->[DataBase]--->[Retriever] -<prompt>-->[model]->[QueryChain]\n",
    "#                                                           \n",
    "# --------------------------------------------------------------------------\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"Your are a helpful AI assistant\"),(\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "output = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm_obj|output\n",
    "response = chain.invoke(\"what is langchain used for?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "159f2b47-ac12-4c69-90c6-22d3ce322cf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "784b182b-8294-4d7f-9316-551954bd0636",
   "metadata": {},
   "source": [
    "pip install llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f9633c3-77b5-417c-a881-257cb18bcb39",
   "metadata": {},
   "source": [
    "# local_ai_search.py\n",
    "\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "LOCAL_FILE = \"emp.csv\"     # <-- Change to your local file\n",
    "TEXT_COLUMN = \"edept\"          # <-- Change to the column name with text\n",
    "OLLAMA_MODEL_NAME = \"gemma:2b\"    # Or 'llama3', 'gemma', etc.\n",
    "\n",
    "# === Load Data ===\n",
    "def load_local_data(path, column):\n",
    "    df = pd.read_csv(path)\n",
    "    return df[column].tolist()\n",
    "\n",
    "# === Embed Data ===\n",
    "def create_embeddings(texts):\n",
    "    embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL_NAME)\n",
    "    vectors = embed_model.get_text_embedding_batch(texts)\n",
    "    vectors = normalize(vectors)  # Normalize for cosine similarity\n",
    "    return vectors\n",
    "\n",
    "# === Search Local Data ===\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatIP(dim)  # Cosine similarity (after normalization)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_local(query, texts, index, embed_model, top_k=3):\n",
    "    query_vec = embed_model.get_text_embedding(query)\n",
    "    query_vec = normalize([query_vec])\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "    results = [texts[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# === General Query with Ollama ===\n",
    "def query_general(query):\n",
    "    llm = Ollama(model=OLLAMA_MODEL_NAME,request_timeout=60)\n",
    "    response = llm.complete(query)\n",
    "    return response\n",
    "\n",
    "# === Unified Query Handler ===\n",
    "def handle_query(query, texts, index, embed_model):\n",
    "    if query.lower().startswith(\"define\"):\n",
    "        return query_general(query)\n",
    "    else:\n",
    "        return search_local(query, texts, index, embed_model)\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    texts = load_local_data(LOCAL_FILE, TEXT_COLUMN)\n",
    "\n",
    "    print(\"Embedding local data...\")\n",
    "    embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL_NAME)\n",
    "    embeddings = create_embeddings(texts)\n",
    "\n",
    "    print(\" Building index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    print(\"\\n Ready! Type your query (e.g., 'define AI' or 'search something')\")\n",
    "    while True:\n",
    "        query = input(\"\\n Your Query: \")\n",
    "        result = handle_query(query, texts, index, embed_model)\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            print(\"\\n Top Matches:\")\n",
    "            for i, r in enumerate(result, 1):\n",
    "                print(f\"{i}. {r}\")\n",
    "        else:\n",
    "            print(\"\\n Definition:\")\n",
    "            print(result)\n",
    "        if(query == 'quit'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac40484-1f34-46e7-9552-12b80170c0a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47ed03-fce0-47a3-9534-164ab0c674bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
