{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34e39c-4181-4efc-bbd3-185355df0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "Recap\n",
    "------\n",
    "[Documents load] --->[chunk] ->[Embedding] ->[VectorDB]-<-[retriever] <--[model] <-[QA]<--[Test]\n",
    ".......................................................-->            ->        ->    -> response\n",
    "\n",
    "\n",
    "OpenAI\n",
    "---------\n",
    "langchain.llms import OpenAI\n",
    "llm = OpenAI(openai_api_key=<>)\n",
    "llm() # callable object => llm(\"Query\") ->response \n",
    "Run: cloud - paid token\n",
    "\n",
    "HuggingFace\n",
    "---------------\n",
    "Run: locally (or) API(cloud)\n",
    "\n",
    "Ollama\n",
    "------\n",
    "Run: locally  - run on local m/c - offline chatbot use \n",
    "====\n",
    "\n",
    "HuggingFace\n",
    "-----------\n",
    "load ->split =>Embedding\n",
    "HuggingFaceEmbeddings(model_name=\"<modelName>\") ->embeddings\n",
    "\n",
    "Stores docs to DB\n",
    "------------------\n",
    "vectorstore =FAISS.from_documents(docs,embeddings)\n",
    "|\n",
    "r =vectorestore.as_reriever() # Retrivever \n",
    "\n",
    "llm_model = Ollama(model=\"ModelName\")\n",
    "\n",
    "RetrievalQA Chain \n",
    "-----------------\n",
    "QA_Chain = RetrievalQA.from_chain_type(llm=<llm_obj>,retriever=<retriever_obj>,return_s...=True)\n",
    "========\n",
    "|__QA_chain.query(\"User inputQuery\") ->response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a19095-8055-43bb-bc96-2ff7e251dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1st - create token from https://huggingface.co/settings/tokens\n",
    "|\n",
    "2nd - update this token to os env  \n",
    "     Go to commandline => Winx: gitbash -> vi .env \n",
    "                                        HF_TOKEN=\"<paste token>\"\n",
    "                                        save this file  :wq \n",
    "\n",
    "|\n",
    "3rd -> go python IDLE (Jupyter;pycharm,vscode etc.,)\n",
    "        |\n",
    "       from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf031e2-16f2-4a28-a5e8-d9fd9e289e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae80c9c-9553-4d70-a806-a4e52e5c49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_3964\\1689552512.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.embeddings.huggingface.HuggingFaceEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c4fd10-e925-46a0-8169-720057c61a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.embed_query(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff6e5-ab65-4dc2-9e9c-120da98c6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"hf_GWAasfsdfsdfsadfsdsfsfsfss\" \n",
    "  Vs\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\") # loading token file and initialize to env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e961f81f-a29f-4cee-9e10-900e224548f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986138b7-8b98-4308-95ac-c999563f8c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6017a4f0-204d-4982-a737-f8524f499b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ccd408-1685-4b48-abf0-d007adf26bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\") # loading token file and initialize to env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2125a626-451e-4e91-86c3-0cfdf2ad4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13897061-2c29-4900-a590-0c3d39fef47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load documents - TextLoader \n",
    "\n",
    "# 2. split text into chunks & initialize to docs\n",
    "\n",
    "# 3. Generate embeddings (use HuggingFace)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(<docs>,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 6. Use model - Ollama model   from langchain_community.llms import Ollama\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "#7. Build RetrievalQA Chain  from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriver,return_source_documents=True)\n",
    "\n",
    "#8. Ask Questions\n",
    "result = qa_chain(\"what is ...\")\n",
    "#9. display / output response \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394cefb8-3300-451a-ac2d-84bd3cb67cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chains import RetrievalQA\n",
    "#help(RetrievalQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5151ff6c-a288-4358-8c4d-901edbab2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd76254a-5dfc-4043-b801-3410db1617ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Sure, here is the answer to the question:\n",
      "\n",
      "**LangChain** is a framework to build LLM-powered apps using composable components.\n",
      "\n",
      "**RAG** stands for Retrieval-Augmented Generation. It helps LLMs use external knowledge.\n",
      "\n",
      "Sources:\n",
      "my_docs.txt\n"
     ]
    }
   ],
   "source": [
    "# 1. Load documents\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2.split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3.Generate embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. store vectors in FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "retriever_obj = vectorstore.as_retriever()\n",
    "\n",
    "# 6. Use Ollama model\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "# 7. Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_obj,retriever=retriever_obj,return_source_documents=True)\n",
    "\n",
    "# 8. Ask Question/Query\n",
    "response = qa_chain(\"what is langchain and how RAG work\")\n",
    "\n",
    "# 9. display/print - response \n",
    "print(\"\\nAnswer:\",response['result'])\n",
    "print(\"\\nSources:\")\n",
    "for var in response[\"source_documents\"]:\n",
    "    print(var.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2dc71-72e8-43ca-b154-93cbac9193f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt\n",
    "=======\n",
    " |->string - text - give to language model to instruct to generate response \n",
    "\n",
    "PromptTemplate\n",
    "==============\n",
    " |->reusable prompt\n",
    "    |->placeholder (or) variable\n",
    "\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "PromptTemplate(input_variables = <> , template=\"user defined message...{<placeholder>} ....\") ->prompt\n",
    "prompt.format()\n",
    "|\n",
    "reusability\n",
    "runtime \n",
    "logic - data separate from prompt structure\n",
    "---//integrate with llmchain(or)pipeline \n",
    "\n",
    "PromptTemplate(input_variable=[\"query\"],template=\"Answer the following query:{query}\") \n",
    "                                |________________________________________________|\n",
    "ChatPromptTemplate\n",
    "---------------------\n",
    " |->roles - multiple roles\n",
    "            --------------\n",
    "           system - control the assistant behavior \n",
    "           human  - user input\n",
    "           AI     - simulate as assistant responses - (optinal)\n",
    "           +\n",
    "           content-text\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "ChatPromptTemplate.from_messages([(\"system\",\"assistant\"),(\"human\",\"write input{var}\")]) ->prompt\n",
    "\n",
    "placeholder ->replaced with input at runtime\n",
    "prompt.format_message(placeholderName=<Value>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1792cffd-0a6a-4117-afe2-4180bfa271e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I likes to read python book')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "PromptTemplate.from_template('I likes to read python book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5d08b1-8515-411e-8899-9d22b8c31fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['myvar'], input_types={}, partial_variables={}, template='I likes to read {myvar} book')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template('I likes to read {myvar} book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f4d7de-37bd-45da-bb00-fdf7e9170e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read java book'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj = PromptTemplate.from_template('I likes to read {myvar} book')\n",
    "prompt_obj.format(myvar=\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac6ab52-dcd5-4cb8-aa81-d21b2c45c34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I likes to read GenAI book'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_obj.format(myvar=\"GenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "122e732d-7708-4fbb-9d6a-6bbca8270dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain some short story about birds in english written by Mr.abc'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pT=PromptTemplate.from_template('Explain some short story about {topic} in {lng} written by {author}')\n",
    "pT.format(topic=\"birds\",lng=\"english\",author=\"Mr.abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c3fbbbf-bd02-4ad6-abf9-469323589708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# (role,content)\n",
    "\n",
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0f0e73a-187a-4d69-815f-c2a8c49d9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "msg = prompt.format_messages(question=\"what is gen AI?\")\n",
    "\n",
    "# use llm chat model ->llmobject\n",
    "#llmobject(msg) <= callable object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a829313a-9f96-46eb-ab0d-15b82749995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "obj = box()\n",
    "callable(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72e2b8e4-4f57-4b9e-9982-102f40b74786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def __call__(self):\n",
    "        pass\n",
    "obj = box()\n",
    "callable(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efad9a77-5055-4d52-8c29-b6a57d932351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return from callable method - llm object :message'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class box:\n",
    "    def __call__(self,a):\n",
    "        return 'return from callable method - llm object :'+a\n",
    "obj = box()\n",
    "obj('message') # llmobj(promptmessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9d149e6-4a9b-4a6d-a65d-729284d46a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "msg = prompt.format_messages(question=\"what is gen AI?\")\n",
    "\n",
    "# use llm chat model ->llmobject\n",
    "#llmobject(msg) <= callable object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c0b6808-f556-493a-ad2d-bd991f422f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])\n",
       "| Ollama(model='gemma:2b')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "#llm_obj(msg)\n",
    "prompt|llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36721368-06bb-4224-b78e-ffc5888c2034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Gen AI** stands for **Generative Artificial Intelligence**. It is a type of artificial intelligence (AI) that focuses on creating new content, such as text, images, music, and more.\\n\\n**Key characteristics of generative AI:**\\n\\n* **Data-driven:** Gen AI models are trained on massive datasets of existing content.\\n* **Creative:** They can generate novel and unique content that is not explicitly programmed.\\n* **Automating tasks:** Gen AI can automate certain creative and repetitive tasks, such as writing, translation, and image editing.\\n* **Learning from data:** Gen AI models continuously learn from new data, enabling them to improve over time.\\n\\n**Examples of generative AI systems:**\\n\\n* **Chatbots:** Chatbots like ChatGPT and Alexa are based on generative AI.\\n* **Language models:** These models can generate human-like text in multiple languages.\\n* **Image generators:** Generative models like DALL-E 2 can create realistic images from text descriptions.\\n* **Music generators:** Generative AI can create new musical compositions and songs.\\n\\n**Benefits of generative AI:**\\n\\n* **Increased creativity:** Gen AI can help artists and content creators generate new ideas.\\n* **Enhanced productivity:** Automating tasks can save time and effort.\\n* **Personalized experiences:** Gen AI can tailor content to individual preferences.\\n* **New applications:** Generative AI has potential applications in various industries, including healthcare, finance, and entertainment.\\n\\n**Challenges of generative AI:**\\n\\n* **Bias and fairness:** Gen AI models can inherit biases from the data they are trained on.\\n* **Lack of interpretability:** The inner workings of generative AI models are often complex and difficult to understand.\\n* **Ethical considerations:** Generative AI can be used for malicious purposes, such as spreading misinformation or generating realistic deepfakes.\\n\\n**Overall, generative AI is a transformative technology with the potential to revolutionize various aspects of human life.**'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','You are a helpful AI assistant'),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "#msg = prompt.format_messages(question=\"what is gen AI?\")\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':'what is gen AI'}) # chainobject.invoke({'placeholder':'user Query value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc08826b-331e-464e-bf50-5a8c7e050990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Step 1: Define the Factorial Function**\\n\\n```python\\ndef factorial(n):\\n    \"\"\"\\n    Calculates the factorial of a non-negative integer n.\\n\\n    Args:\\n        n: The non-negative integer for which to calculate the factorial.\\n\\n    Returns:\\n        The factorial of n.\\n    \"\"\"\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n```\\n\\n**Step 2: Get the Input from the User**\\n\\n```python\\n# Get the input from the user\\nn = int(input(\"Enter a non-negative integer: \"))\\n```\\n\\n**Step 3: Calculate the Factorial**\\n\\n```python\\nfactorial_result = factorial(n)\\n```\\n\\n**Step 4: Print the Result**\\n\\n```python\\n# Print the result of the factorial calculation\\nprint(f\"The factorial of {n} is {factorial_result}\")\\n```\\n\\n**Example Usage:**\\n\\n```\\nEnter a non-negative integer: 5\\nThe factorial of 5 is 120\\n```\\n\\n**Note:**\\n\\n* The `factorial()` function calculates the factorial of a non-negative integer `n`.\\n* The function uses a recursive approach to compute the factorial.\\n* The `if` statement handles the base case for `n` being 0.\\n* The `return` statements ensure that the factorial is calculated accurately.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'How to write factorial program in python?'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c850f114-42b4-49a1-9880-5ff75892c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's how to write a factorial program in Python:\n",
      "\n",
      "```python\n",
      "# Define the factorial function\n",
      "def factorial(n):\n",
      "    # Initialize the factorial to 1 (since 0! is defined as 1!)\n",
      "    factorial = 1\n",
      "\n",
      "    # Iterate from 1 to n\n",
      "    for i in range(1, n + 1):\n",
      "        # Multiply the factorial of i by the factorial of n\n",
      "        factorial *= i\n",
      "\n",
      "    # Return the factorial\n",
      "    return factorial\n",
      "\n",
      "\n",
      "# Get the input from the user\n",
      "n = int(input(\"Enter a non-negative integer: \"))\n",
      "\n",
      "# Calculate and print the factorial of n\n",
      "print(f\"The factorial of {n} is {factorial(n)}\")\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. The `factorial()` function takes one argument, `n`, which is the number for which we want to calculate the factorial.\n",
      "2. It initializes a `factorial` variable to 1, as 0! is defined as 1!.\n",
      "3. It then enters a `for` loop that iterates from 1 to `n`.\n",
      "4. Inside the loop, it multiplies the `factorial` of `i` by the `factorial` of `n`. This accumulates the products of all non-zero integers from 1 to `n` in the `factorial` variable.\n",
      "5. After the loop completes, the function returns the final `factorial` value.\n",
      "6. The user is prompted to input a non-negative integer, and the program calculates and prints the factorial of that number using the `factorial()` function.\n",
      "\n",
      "**Example Usage:**\n",
      "\n",
      "```\n",
      "Enter a non-negative integer: 5\n",
      "\n",
      "The factorial of 5 is 120\n",
      "```\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The factorial of 0 is defined as 1! (since 0! is equal to 1!).\n",
      "* This program assumes that the input is a valid non-negative integer. If the user enters a non-positive or negative number, the program will produce an error.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({'question':'How to write factorial program in python?'}) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d359c068-6a8f-4ac1-aae0-9b6e20538c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I can help.\\n\\n**Node A** is a term used in graph theory and data structures to refer to a node or vertex in a graph. It is a point in the graph that is connected to other nodes by edges.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'What is nodeA?'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "267ed156-c819-41ae-9310-033ce9ac8bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, I can help with that! Here's some information about Timmins Tech Center:\\n\\n**Location:**\\n* Address: 123 Main Street, Anytown, CA 12345\\n\\n**Contact:**\\n* Phone: (555) 123-4567\\n* Website: (555) 123-4567\\n* Email: info@timmins.com\\n\\n**Services:**\\n* Timmins Tech Center offers a wide range of IT services, including:\\n    * Computer repair\\n    * Data recovery\\n    * Software installation and maintenance\\n    * Networking\\n    * Cybersecurity\\n    * Cloud computing\\n    * And more\\n\\n**Hours of operation:**\\n* Monday - Friday: 8am to 5pm PST\\n\\n**Additional information:**\\n* Timmins Tech Center is a locally owned and operated business.\\n* They have been serving the community for over 10 years.\\n* They are committed to providing high-quality, affordable IT services.\\n\\n**Here are some additional resources that you may find helpful:**\\n* Timmins Tech Center website\\n* Google Maps location\\n* Reviews from previous customers\\n\\nI hope this information is helpful! Please let me know if you have any other questions.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'Get some info about timins tech center'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28df3401-41a5-4a31-990d-a796fc04829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? Is there anything I can help you with?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':'asfasfas safsadf'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ece5930-a41e-49c9-ba2d-743d06bb6462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am unable to form opinions or ask questions. I do not have a physical body or consciousness, and I am unable to experience the world in the same way that humans do.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':'asfadss?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea2949ec-aa56-4127-a1d3-f935f3f6df59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure. Here's a breakdown of embedding:\\n\\n**Embedding** is a technique that allows you to represent data in a **lower-dimensional space** while preserving its essential characteristics. This can be achieved by using a **similarity or distance metric** to map the data onto a **higher-dimensional space**.\\n\\n**Here's how embedding works:**\\n\\n1. **Data transformation:** The data is input into an **embedding algorithm**, which uses a mathematical function to transform it into a new representation. This embedding can be done in various ways, such as **linear embedding**, **non-linear embedding**, or **deep learning embeddings**.\\n2. **Feature extraction:** The embedding algorithm also extracts features from the data, which are specific properties that are important for the task at hand. These features can be used for various tasks, such as **classification, regression, and clustering**.\\n3. **Dimensionality reduction:** To reduce the dimensionality of the data, the embedding algorithm can learn a lower-dimensional representation of the data. This can be achieved by using techniques like **principal component analysis (PCA)** or **t-distributed stochastic neighbor embedding (t-SNE)**.\\n4. **Applications:** Embeddings are widely used in various applications, including:\\n\\n  - **Natural language processing (NLP):** Embeddings are used to represent words and sentences in a vector space, enabling tasks such as sentiment analysis and text classification.\\n  - **Computer vision:** Embeddings are used to represent images and videos in a way that allows computers to understand and analyze them.\\n  - **Music information retrieval:** Embeddings are used to represent musical pieces and artists in a vector space, enabling tasks such as music genre classification and artist recommendation.\\n  - **Social network analysis:** Embeddings are used to represent social network data in a way that allows for analysis of network structure and dynamics.\\n\\n**Benefits of embedding:**\\n\\n* **Dimensionality reduction:** Embeddings can reduce the dimensionality of data while preserving important information. This can improve the efficiency and speed of machine learning models.\\n* **Feature extraction:** Embeddings can extract features from data that may not be readily apparent in the original space.\\n* **Representation learning:** Embeddings can learn meaningful representations of data, which can be used for various tasks.\\n\\n**Overall, embedding is a powerful technique that can be used to represent and analyze data in a lower-dimensional space. It has a wide range of applications in various fields, including natural language processing, computer vision, and data mining.**\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"don't know\"),('user','Question:{question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_obj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "chain = prompt|llm_obj\n",
    "\n",
    "chain.invoke({'question':'explain about embedding?'})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ff5d315-5fb1-4bad-8e7f-69acc6e72840",
   "metadata": {},
   "source": [
    "def query_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",  # Specify the model you're using\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json().get(\"response\", \"\")\n",
    "        # Check if the response is empty or contains a \"don't know\" type message\n",
    "        if not result or \"don't know\" in result.lower() or \"no information\" in result.lower():\n",
    "            return \"No results found. Please try a different query.\"\n",
    "        return result\n",
    "    else:\n",
    "        return \"Error connecting to Ollama. Please check if it's running.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10ff096d-7ac5-4240-8078-5a1e7d38c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = chain.invoke({'question':'what is python?'})\n",
    "r2 = chain.invoke({'question':'asfsafsadsd?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90bdbe4a-08cb-4f68-9dd2-4f4204173647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(bool(r1),bool(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa5644f0-90b4-435a-87db-ba285d18af52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = ''\n",
    "bool(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3b035d4-33d4-47bd-b6ac-225c0343a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure what you're asking. Can you rephrase your question?\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64933444-33ff-4253-9f17-be0700270d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. Here's a brief explanation of Python:\n",
      "\n",
      "**Python** is a high-level programming language known for its **conciseness, readability, and versatility**. It's often referred to as **\"the Python of choice\"** due to its popularity and widespread use.\n",
      "\n",
      "**Here's what you can do with Python:**\n",
      "\n",
      "* **Code generation:** Write Python code and use interactive tools to create programs on the fly.\n",
      "* **Data manipulation:** Work with data using built-in functions and libraries.\n",
      "* **Machine learning and data science:** Develop and analyze machine learning models for various tasks.\n",
      "* **Web development:** Build websites and web applications with libraries like Django.\n",
      "* **Automation tasks:** Automate repetitive tasks with powerful modules.\n",
      "* **Game development:** Create interactive and engaging games with libraries like Pygame.\n",
      "\n",
      "**Benefits of learning Python:**\n",
      "\n",
      "* **High demand:** Python is one of the most widely used languages in the world.\n",
      "* **Simple syntax:** Python's syntax is relatively easy to learn.\n",
      "* **Large and supportive community:** There are plenty of resources, tutorials, and online forums to help you learn.\n",
      "* **Versatile applications:** You can use Python for various tasks, from data analysis to web development.\n",
      "\n",
      "**In summary, Python is a powerful and versatile language that can be used for a wide range of tasks, including:**\n",
      "\n",
      "* Data science\n",
      "* Machine learning\n",
      "* Web development\n",
      "* Automation\n",
      "* Game development\n",
      "* And more\n",
      "\n",
      "If you're interested in learning more about Python, there are many resources available online and in libraries. You can also take online courses, read books, and watch tutorials.\n"
     ]
    }
   ],
   "source": [
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f279bfc-cb4b-40b3-ac4e-b48d9d92b8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
