{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad9aa73-5472-4c0b-a915-f65d42627ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loaded Document(s) ---\n",
      "\n",
      "[Doc 1]:\n",
      "Langchain is a powerful framework for building applications with large language models.\n",
      "core components of langchains chains,agent,memory,retrievers and other tools.\n",
      "Guido vanrosum is an author of python programming\n",
      "\n",
      "\n",
      "--- Split Chunks ---\n",
      "\n",
      "[Chunk 1]:\n",
      "Langchain is a powerful framework for building applications with large language models.\n",
      "core components of langchains chains,agent,memory,retrievers and other tools.\n",
      "Guido vanrosum is an author of python programming\n",
      "\n",
      "--- Query: 'author' ---\n",
      "\n",
      "Found 1 result(s).\n",
      "\n",
      "[Match 1]:\n",
      "Langchain is a powerful framework for building applications with large language models.\n",
      "core components of langchains chains,agent,memory,retrievers and other tools.\n",
      "Guido vanrosum is an author of python programming\n"
     ]
    }
   ],
   "source": [
    "## 1st - result\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Step 1: Load the file\n",
    "loader = TextLoader('my_file.txt')\n",
    "documents = loader.load()\n",
    "print(\"\\n--- Loaded Document(s) ---\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\n[Doc {i+1}]:\\n{doc.page_content}\")\n",
    "\n",
    "# Step 2: Split the text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=20, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(\"\\n--- Split Chunks ---\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[Chunk {i+1}]:\\n{doc.page_content}\")\n",
    "\n",
    "# Step 3: Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(model='gemma:2b')\n",
    "\n",
    "# Step 4: Create FAISS index\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 5: Perform similarity search\n",
    "query = \"author\"\n",
    "print(f\"\\n--- Query: '{query}' ---\")\n",
    "results = db.similarity_search(query)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nFound {len(results)} result(s).\")\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"\\n[Match {i+1}]:\\n{res.page_content}\")\n",
    "else:\n",
    "    print(\"\\nNo matches found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165108c1-4327-4092-8473-6ce5a4b54569",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama_index\n",
      "  Downloading llama_index-0.12.52-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)\n",
      "Collecting llama-index-cli<0.5,>=0.4.2 (from llama_index)\n",
      "  Downloading llama_index_cli-0.4.4-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.52.post1 (from llama_index)\n",
      "  Downloading llama_index_core-0.12.52.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama_index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.5.3-py3-none-any.whl.metadata (441 bytes)\n",
      "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
      "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_file-0.4.11-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama_index) (3.9.1)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
      "  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.11.10)\n",
      "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.2.13)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (2025.3.2)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.28.1)\n",
      "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading llama_index_workflows-1.2.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.1.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (11.1.0)\n",
      "Requirement already satisfied: platformdirs in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (4.3.7)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.32.3)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.0.39)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (9.0.0)\n",
      "Collecting tiktoken>=0.7.0 (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.18.0)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading griffe-1.9.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.1.6)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (5.9.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.5)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading llama_index_instrumentation-0.3.0-py3-none-any.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.52.post1->llama_index) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13,>=0.12.52.post1->llama_index) (0.4.6)\n",
      "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.52.post1->llama_index)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.52-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.52 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.52-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-cloud-services>=0.6.52->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (8.1.8)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from llama-cloud-services>=0.6.52->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.1.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama_index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\karth\\appdata\\roaming\\python\\python313\\site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.52.post1->llama_index) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.13,>=0.12.52.post1->llama_index) (3.0.2)\n",
      "Downloading llama_index-0.12.52-py3-none-any.whl (7.1 kB)\n",
      "Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_cli-0.4.4-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.12.52.post1-py3-none-any.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.6/7.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.1/7.6 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.0/7.6 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.6 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.8/7.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.5.3-py3-none-any.whl (3.4 kB)\n",
      "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
      "Downloading llama_index_readers_file-0.4.11-py3-none-any.whl (41 kB)\n",
      "Downloading llama_index_workflows-1.2.0-py3-none-any.whl (37 kB)\n",
      "Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "   ---------------------------------------- 0.0/764.4 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/764.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 764.4/764.4 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading jiter-0.10.0-cp313-cp313-win_amd64.whl (205 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.8.0-py3-none-any.whl (16 kB)\n",
      "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
      "Downloading llama_index_instrumentation-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_parse-0.6.52-py3-none-any.whl (4.9 kB)\n",
      "Downloading llama_cloud_services-0.6.52-py3-none-any.whl (48 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading tiktoken-0.9.0-cp313-cp313-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.7 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/894.7 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 894.7/894.7 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading griffe-1.9.0-py3-none-any.whl (137 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, setuptools, jiter, griffe, deprecated, aiosqlite, tiktoken, openai, llama-index-instrumentation, llama-cloud, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   -- -------------------------------------  2/27 [setuptools]\n",
      "   ----- ----------------------------------  4/27 [griffe]\n",
      "   ----- ----------------------------------  4/27 [griffe]\n",
      "   ----- ----------------------------------  4/27 [griffe]\n",
      "   ---------- -----------------------------  7/27 [tiktoken]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ----------- ----------------------------  8/27 [openai]\n",
      "   ------------- --------------------------  9/27 [llama-index-instrumentation]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   -------------- ------------------------- 10/27 [llama-cloud]\n",
      "   ----------------- ---------------------- 12/27 [llama-index-workflows]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   ------------------- -------------------- 13/27 [llama-index-core]\n",
      "   -------------------- ------------------- 14/27 [llama-index-readers-file]\n",
      "   ---------------------- ----------------- 15/27 [llama-index-llms-openai]\n",
      "   -------------------------- ------------- 18/27 [llama-cloud-services]\n",
      "   ---------------------------- ----------- 19/27 [llama-parse]\n",
      "   -------------------------------- ------- 22/27 [llama-index-agent-openai]\n",
      "   ---------------------------------------- 27/27 [llama_index]\n",
      "\n",
      "Successfully installed aiosqlite-0.21.0 banks-2.2.0 deprecated-1.2.18 dirtyjson-1.0.8 griffe-1.9.0 jiter-0.10.0 llama-cloud-0.1.35 llama-cloud-services-0.6.52 llama-index-agent-openai-0.4.12 llama-index-cli-0.4.4 llama-index-core-0.12.52.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.8.0 llama-index-instrumentation-0.3.0 llama-index-llms-openai-0.4.7 llama-index-multi-modal-llms-openai-0.5.3 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.11 llama-index-readers-llama-parse-0.4.0 llama-index-workflows-1.2.0 llama-parse-0.6.52 llama_index-0.12.52 openai-1.97.1 setuptools-80.9.0 striprtf-0.0.26 tiktoken-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3826499f-93fc-4fb7-90ec-316da48eb93d",
   "metadata": {},
   "source": [
    "pip install llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "389fa2cb-5127-48f9-9b27-59698deff196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Embedding local data...\n",
      " Building index...\n",
      "\n",
      " Ready! Type your query (e.g., 'define AI' or 'search something')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Your Query:  define what is langchain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Definition:\n",
      "Sure, here's the definition of a LangChain:\n",
      "\n",
      "**A LangChain is a generative language model that uses a unique approach to language modeling based on the concept of **repetition**. It's different from traditional language models that rely on statistical relationships between words or sequences of words. Instead, LangChain uses a neural network to repeat the same sequences of characters or words with a specific ratio or frequency. This approach allows LangChain to generate novel and unique text, even when no training data is present.**\n",
      "\n",
      "Here are some additional key points about LangChain:\n",
      "\n",
      "* It is developed by Google.\n",
      "* It is a large language model (LLM) with a massive dataset of trigrams (three-character sequences).\n",
      "* It uses a self-attention mechanism to learn the relationships between words.\n",
      "* It can generate text in various languages.\n",
      "* It is a very powerful tool for language generation and translation.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Your Query:  what is langchain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top Matches:\n",
      "1. admin\n",
      "2. hr\n",
      "3. HR\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Your Query:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top Matches:\n",
      "1. admin\n",
      "2. hr\n",
      "3. HR\n"
     ]
    }
   ],
   "source": [
    "# local_ai_search.py\n",
    "\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "LOCAL_FILE = \"emp.csv\"     # <-- Change to your local file\n",
    "TEXT_COLUMN = \"edept\"          # <-- Change to the column name with text\n",
    "OLLAMA_MODEL_NAME = \"gemma:2b\"    # Or 'llama3', 'gemma', etc.\n",
    "\n",
    "# === Load Data ===\n",
    "def load_local_data(path, column):\n",
    "    df = pd.read_csv(path)\n",
    "    return df[column].tolist()\n",
    "\n",
    "# === Embed Data ===\n",
    "def create_embeddings(texts):\n",
    "    embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL_NAME)\n",
    "    vectors = embed_model.get_text_embedding_batch(texts)\n",
    "    vectors = normalize(vectors)  # Normalize for cosine similarity\n",
    "    return vectors\n",
    "\n",
    "# === Search Local Data ===\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatIP(dim)  # Cosine similarity (after normalization)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_local(query, texts, index, embed_model, top_k=3):\n",
    "    query_vec = embed_model.get_text_embedding(query)\n",
    "    query_vec = normalize([query_vec])\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "    results = [texts[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# === General Query with Ollama ===\n",
    "def query_general(query):\n",
    "    llm = Ollama(model=OLLAMA_MODEL_NAME,request_timeout=60)\n",
    "    response = llm.complete(query)\n",
    "    return response\n",
    "\n",
    "# === Unified Query Handler ===\n",
    "def handle_query(query, texts, index, embed_model):\n",
    "    if query.lower().startswith(\"define\"):\n",
    "        return query_general(query)\n",
    "    else:\n",
    "        return search_local(query, texts, index, embed_model)\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    texts = load_local_data(LOCAL_FILE, TEXT_COLUMN)\n",
    "\n",
    "    print(\"Embedding local data...\")\n",
    "    embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL_NAME)\n",
    "    embeddings = create_embeddings(texts)\n",
    "\n",
    "    print(\" Building index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    print(\"\\n Ready! Type your query (e.g., 'define AI' or 'search something')\")\n",
    "    while True:\n",
    "        query = input(\"\\n Your Query: \")\n",
    "        result = handle_query(query, texts, index, embed_model)\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            print(\"\\n Top Matches:\")\n",
    "            for i, r in enumerate(result, 1):\n",
    "                print(f\"{i}. {r}\")\n",
    "        else:\n",
    "            print(\"\\n Definition:\")\n",
    "            print(result)\n",
    "        if(query == 'quit'):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ebde68-2fa1-47c3-828b-5226da841940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading data...\n",
      "ðŸ§  Embedding local data...\n",
      "ðŸ“š Building index...\n",
      "\n",
      "âœ… Ready! Type your query (e.g., 'define AI' or 'search something')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your Query:  who is prisident of india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Top Matches:\n",
      "1. admin\n",
      "2. hr\n",
      "3. HR\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your Query:  define who is prisident of india\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Definition:\n",
      "Sure. Here's a definition of the President of India:\n",
      "\n",
      "**President of India**\n",
      "\n",
      "The President is the constitutional head of India and is the sovereign head of the Republic. The President is elected by the Lok Sabha, the House of People, for a five-year term.\n",
      "\n",
      "**Key Responsibilities of the President:**\n",
      "\n",
      "* Presides over the Union of India and is the supreme authority in the country.\n",
      "* Represents India internationally, both formally and informally.\n",
      "* Appoints the Prime Minister and other members of the Council of Ministers.\n",
      "* Promotes the welfare of the people and the nation.\n",
      "* Signs bills into law.\n",
      "* Grants pardon and clemency.\n",
      "* Represents India at official ceremonies and functions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your Query:  I want to know about president name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Top Matches:\n",
      "1. admin\n",
      "2. hr\n",
      "3. HR\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your Query:  define president name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Definition:\n",
      "Sure, here is the definition of the `president` name:\n",
      "\n",
      "**President Name**\n",
      "\n",
      "The president's name is the official title and legal designation of the head of state in a country. In the United States, the President of the United States is the head of government and is typically referred to as \"President.\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your Query:  define president of india person name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Definition:\n",
      "A president of India is a person who is the head of the Indian government. The current president of India is <current_president_name>, who took office on <date>.\n",
      "\n",
      "**Additional information:**\n",
      "\n",
      "* The President of India is the supreme authority in India and holds significant constitutional powers.\n",
      "* The President is elected by the Electoral College, which is composed of all elected members of the Indian Parliament.\n",
      "* The President can serve a maximum of two terms, which is typically for a period of five years each.\n",
      "* The current President is Ram Nath Kovind.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your Query:  Thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Top Matches:\n",
      "1. admin\n",
      "2. HR\n",
      "3. hr\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Ready! Type your query (e.g., \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefine AI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch something\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ” Your Query: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m     result \u001b[38;5;241m=\u001b[39m handle_query(query, texts, index, embed_model)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# local_ai_search.py\n",
    "\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "LOCAL_FILE = \"emp.csv\"     # <-- Change to your local file\n",
    "TEXT_COLUMN = \"edept\"          # <-- Change to the column name with text\n",
    "OLLAMA_MODEL_NAME = \"gemma:2b\"    # Or 'llama3', 'gemma', etc.\n",
    "\n",
    "# === Load Data ===\n",
    "def load_local_data(path, column):\n",
    "    df = pd.read_csv(path)\n",
    "    return df[column].tolist()\n",
    "\n",
    "# === Embed Data ===\n",
    "def create_embeddings(texts):\n",
    "    embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL_NAME)\n",
    "    vectors = embed_model.get_text_embedding_batch(texts)\n",
    "    vectors = normalize(vectors)  # Normalize for cosine similarity\n",
    "    return vectors\n",
    "\n",
    "# === Search Local Data ===\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatIP(dim)  # Cosine similarity (after normalization)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_local(query, texts, index, embed_model, top_k=3):\n",
    "    query_vec = embed_model.get_text_embedding(query)\n",
    "    query_vec = normalize([query_vec])\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "    results = [texts[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# === General Query with Ollama ===\n",
    "def query_general(query):\n",
    "    llm = Ollama(model=OLLAMA_MODEL_NAME,request_timeout=60)\n",
    "    response = llm.complete(query)\n",
    "    return response\n",
    "\n",
    "# === Unified Query Handler ===\n",
    "def handle_query(query, texts, index, embed_model):\n",
    "    if query.lower().startswith(\"define\"):\n",
    "        return query_general(query)\n",
    "    else:\n",
    "        return search_local(query, texts, index, embed_model)\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ”„ Loading data...\")\n",
    "    texts = load_local_data(LOCAL_FILE, TEXT_COLUMN)\n",
    "\n",
    "    print(\"ðŸ§  Embedding local data...\")\n",
    "    embed_model = OllamaEmbedding(model_name=OLLAMA_MODEL_NAME)\n",
    "    embeddings = create_embeddings(texts)\n",
    "\n",
    "    print(\"ðŸ“š Building index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    print(\"\\nâœ… Ready! Type your query (e.g., 'define AI' or 'search something')\")\n",
    "    while True:\n",
    "        query = input(\"\\nðŸ” Your Query: \")\n",
    "        result = handle_query(query, texts, index, embed_model)\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            print(\"\\nðŸ”Ž Top Matches:\")\n",
    "            for i, r in enumerate(result, 1):\n",
    "                print(f\"{i}. {r}\")\n",
    "        else:\n",
    "            print(\"\\nðŸ“– Definition:\")\n",
    "            print(result)\n",
    "        if(query == 'quit'):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85eb8f3b-8aa9-43fb-81ec-9995a10fe04b",
   "metadata": {},
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Load text documents from a directory\n",
    "documents = SimpleDirectoryReader(\"./Demo\").load_data()\n",
    "\n",
    "# Use Ollama for LLM and embeddings (no API key needed)\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "embed_model = OllamaEmbedding(model_name=\"gemma:2b\")\n",
    "\n",
    "# Build vector index\n",
    "index = VectorStoreIndex.from_documents(documents, llm=llm, embed_model=embed_model)\n",
    "\n",
    "# Ask questions\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Summarize the content.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d078c38-cde8-444c-830e-4dcfc68996f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "dog is in the yard\n",
      "cat is on the mat\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 1. Prepare data\n",
    "texts = [\"cat is on the mat\", \"dog is in the yard\", \"fish swims in water\"]\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectors = model.encode(texts)\n",
    "vectors = normalize(vectors)  # for cosine similarity\n",
    "\n",
    "# 2. Build FAISS index\n",
    "dim = vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # IP = Inner Product (used for cosine)\n",
    "index.add(vectors)\n",
    "\n",
    "# 3. Search query\n",
    "query = \"Where is the dog?\"\n",
    "query_vector = model.encode([query])\n",
    "query_vector = normalize(query_vector)\n",
    "\n",
    "scores, indices = index.search(query_vector, k=2)\n",
    "print(\"Top results:\")\n",
    "for i in indices[0]:\n",
    "    print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466a356c-84c1-4c5e-9f75-0c506e05e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "fish swims in water\n",
      "cat is on the mat\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 1. Prepare data\n",
    "texts = [\"cat is on the mat\", \"dog is in the yard\", \"fish swims in water\"]\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectors = model.encode(texts)\n",
    "vectors = normalize(vectors)  # for cosine similarity\n",
    "\n",
    "# 2. Build FAISS index\n",
    "dim = vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # IP = Inner Product (used for cosine)\n",
    "index.add(vectors)\n",
    "\n",
    "# 3. Search query\n",
    "query = \"Where is the fish?\"\n",
    "query_vector = model.encode([query])\n",
    "query_vector = normalize(query_vector)\n",
    "\n",
    "scores, indices = index.search(query_vector, k=2)\n",
    "print(\"Top results:\")\n",
    "for i in indices[0]:\n",
    "    print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d24c110d-6a31-4e73-a2be-2845333b92c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "cat is on the mat\n",
      "dog is in the yard\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 1. Prepare data\n",
    "texts = [\"cat is on the mat\", \"dog is in the yard\", \"fish swims in water\"]\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectors = model.encode(texts)\n",
    "vectors = normalize(vectors)  # for cosine similarity\n",
    "\n",
    "# 2. Build FAISS index\n",
    "dim = vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # IP = Inner Product (used for cosine)\n",
    "index.add(vectors)\n",
    "\n",
    "# 3. Search query\n",
    "query = \"Who is PM of india?\"\n",
    "query_vector = model.encode([query])\n",
    "query_vector = normalize(query_vector)\n",
    "\n",
    "scores, indices = index.search(query_vector, k=2)\n",
    "print(\"Top results:\")\n",
    "for i in indices[0]:\n",
    "    print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514d18a8-bd06-473d-8fb7-55512b05245d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get embeddings for multiple queries\n",
    "embedding = OllamaEmbeddings(model='gemma:2b')\n",
    "sentences = [\"Hello\", \"Hi\", \"Goodbye\", \"Farewell\", \"Dog\", \"Cat\", \"Computer\"]\n",
    "vectors = [embedding.embed_query(s) for s in sentences]\n",
    "\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, point in enumerate(reduced):\n",
    "    plt.scatter(point[0], point[1])\n",
    "    plt.text(point[0]+0.01, point[1]+0.01, sentences[i])\n",
    "plt.title(\"2D PCA of Sentence Embeddings\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a116227a-bdb6-4acd-8043-b95dc695a87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity search for: 'Greetings'\n",
      "\n",
      "Hello: 0.8503\n",
      "Goodbye: 0.8448\n",
      "Farewell: 0.8140\n",
      "Hi: 0.7065\n",
      "Dog: 0.5034\n",
      "Cat: 0.4680\n",
      "Computer: 0.4470\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define sentences\n",
    "sentences = [\"Hello\", \"Hi\", \"Goodbye\", \"Farewell\", \"Dog\", \"Cat\", \"Computer\"]\n",
    "vectors = [embedding.embed_query(s) for s in sentences]\n",
    "\n",
    "# Query\n",
    "query = \"Greetings\"\n",
    "query_vec = embedding.embed_query(query)\n",
    "\n",
    "# Compute similarities\n",
    "sims = cosine_similarity([query_vec], vectors)[0]\n",
    "\n",
    "# Pair and sort\n",
    "ranked = sorted(zip(sentences, sims), key=lambda x: -x[1])\n",
    "print(f\"Similarity search for: '{query}'\\n\")\n",
    "for sent, score in ranked:\n",
    "    print(f\"{sent}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bff3fbe-511e-4b8d-900a-49548845526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(model='gemma:2b')\n",
    "\n",
    "r = embedding.embed_query('Hello')\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f8f493-1f11-4856-b724-b526090cc544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "r = embedding.embed_query('Welcome to Gen AI')\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1541c6-1124-4d58-aebc-94d363396b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "r = embedding.embed_query('')\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84891b6a-9cda-41bb-bb5d-429ad8b8b972",
   "metadata": {},
   "source": [
    "len(r) is 2048, which is the fixed embedding size used by the gemma:2b model.\n",
    "\n",
    "This size is set by the architecture of the embedding model and is independent \n",
    "of the length of the input string\n",
    "\n",
    "An embedding is a numerical representation of data (like a word, sentence, or paragraph) \n",
    "in a vector space â€” basically, a list of numbers that captures the meaning or context of that data.\n",
    "[0.13, -0.57, 0.22, ..., 0.45]  # A list of 2048 numbers\n",
    "This vector is what the model uses to understand and compare the meanings of different texts.\n",
    "\n",
    "\n",
    " What does 2048-dimensional mean?\n",
    "It means the embedding is a vector with 2048 numbers (dimensions).\n",
    "\n",
    "Each number is a feature that captures some aspect of the input's meaning, syntax, or context.\n",
    "\n",
    "These features are learned during the training of the model (like gemma:2b) and are not human-interpretable individually â€” but together, they create a useful representation of the input.\n",
    "\n",
    "For example:\n",
    "\n",
    "# One sentence â†’ embedding\n",
    "\"Hello\" â†’ [0.01, -0.23, 0.94, ..., 0.75]  # length 2048\n",
    "\"Goodbye\" â†’ [0.02, -0.19, 0.88, ..., 0.72]  # also length 2048\n",
    "The closer these two vectors are in space (e.g. by cosine similarity), the more semantically related the inputs are.\n",
    "\n",
    "ðŸ” Visual Analogy: \"Coordinates in Meaning Space\"\n",
    "Imagine you're on Earth. You can describe any point using:\n",
    "\n",
    "2D: latitude & longitude (like a map).\n",
    "\n",
    "3D: latitude, longitude, and altitude.\n",
    "\n",
    "Now imagine instead of Earth, you're in a very high-dimensional space (2048D!), where each dimension represents some hidden meaning like:\n",
    "\n",
    "Is the sentence about animals?\n",
    "\n",
    "Is it happy or sad?\n",
    "\n",
    "Is it formal or casual?\n",
    "\n",
    "Is it a question?\n",
    "\n",
    "Is it related to technology?\n",
    "\n",
    "... and 2043 more subtle features.\n",
    "\n",
    "So an embedding is like giving coordinates to a sentence in this \"semantic universe\".\n",
    "\n",
    " \"Hello\" becomes a point in this space â€” like saying:\n",
    "[0.123, -0.543, 0.991, ..., 0.019] (2048 numbers)\n",
    "\n",
    "Now:\n",
    "\n",
    "\"Hello\" and \"Hi\" will be close together in this space.\n",
    "\n",
    "\"Hello\" and \"Cat\" might be farther apart.\n",
    "\n",
    "\"Hello\" and \"Goodbye\" could still be somewhat close (they're both greetings).\n",
    "     |                             .\n",
    "     |         \"Hello\"    .\n",
    "     |                   .     \"Hi\"\n",
    "     |     \"Goodbye\"  .\n",
    "     |                          \"Dog\"\n",
    "     |                      .\n",
    "     |         \"Computer\"\n",
    "     -----------------------------------\n",
    "\n",
    "\n",
    "Why use Embeddings?\n",
    "Embeddings turn human language into dense, meaningful numbers, so machines can:\n",
    "Compare meanings (semantic similarity)\n",
    "Retrieve relevant content (search)\n",
    "Cluster similar texts (grouping)\n",
    "Power chatbots (context-aware responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4267e0a-dab3-417b-975a-c48fd7c847df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Hello\", \"Hi\", \"Goodbye\", \"Dog\", \"Cat\", \"Computer\"]\n",
    "vectors = [embedding.embed_query(s) for s in sentences]\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427a4570-b5fe-4448-9b4a-43a853dcf2d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Most relevant document:\n",
      "Apples are usually red or green.\n",
      "\n",
      "ðŸ¤– LLM Answer:\n",
      "The context does not provide information about the color of bananas, so I cannot answer this question from the context.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOllama\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Define your documents\n",
    "documents = [\n",
    "    \"Apples are usually red or green.\",\n",
    "    \"Bananas are yellow and rich in potassium.\",\n",
    "    \"Cats are popular household pets.\",\n",
    "    \"Python is a widely used programming language.\",\n",
    "    \"Water freezes at 0 degrees Celsius.\"\n",
    "]\n",
    "\n",
    "# 2. Create embedding model\n",
    "embedding = OllamaEmbeddings(model='gemma:2b')\n",
    "\n",
    "# 3. Embed the documents\n",
    "vectorstore = FAISS.from_texts(documents, embedding)\n",
    "\n",
    "# 4. Define a query\n",
    "query = \"What color are Bananas?\"\n",
    "\n",
    "# 5. Embed and retrieve most similar documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "top_doc = relevant_docs[0].page_content\n",
    "\n",
    "print(\"ðŸ“„ Most relevant document:\")\n",
    "print(top_doc)\n",
    "\n",
    "# 6. Use LangChain prompt with ChatOllama (or any LLM)\n",
    "llm = ChatOllama(model='gemma:2b')  # or ChatOpenAI if you're using OpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert AI assistant. Use ONLY the context below to answer the user's question. If the answer is in the context, respond clearly. Do NOT say \"the context does not provide...\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in one short, factual sentence:\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# 7. Run the chain\n",
    "response = chain.invoke({\"context\": top_doc, \"question\": query})\n",
    "print(\"\\nðŸ¤– LLM Answer:\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4256f-0d25-4d29-9684-f7eb23674f5f",
   "metadata": {},
   "source": [
    "Similarity Search means:\n",
    "==========================\n",
    "\"Given a query, find the most semantically similar content \n",
    "rom a set of documents or texts.\"\n",
    "\n",
    "Rather than matching exact words (like keyword search),it matches meanings using embeddings.\n",
    "\n",
    "Step 1. Embed Your Documents\n",
    "------------------------------\n",
    "You first convert each document or chunk into an embedding vector using \n",
    "the same model you use for queries.\n",
    "docs = [\"Apples are red\", \"Bananas are yellow\", \"Python is a programming language\"]\n",
    "doc_embeddings = [embedding.embed_query(doc) for doc in docs]\n",
    "Each document is now a 2048-dimensional vector.\n",
    "\n",
    "Step 2. Embed the Query\n",
    "-------------------------\n",
    "Do the same with your user's input or search query.\n",
    "query = \"What color are bananas?\"\n",
    "query_vector = embedding.embed_query(query)\n",
    "\n",
    "Step 3. Compute Similarities\n",
    "-----------------------------\n",
    "Compare the query vector to each document vector using cosine similarity (or another similarity metric).\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Compute similarities\n",
    "sims = cosine_similarity([query_vector], doc_embeddings)[0]\n",
    "\n",
    "# Pair results with original docs\n",
    "results = list(zip(docs, sims))\n",
    "\n",
    "# Sort by similarity score (highest first)\n",
    "sorted_results = sorted(results, key=lambda x: -x[1])\n",
    "for doc, score in sorted_results:\n",
    "    print(f\"{score:.3f}: {doc}\")\n",
    "\n",
    "Step 4. Use with LLM\n",
    "-----------------------\n",
    "Once you retrieve the most relevant document(s), you can pass them to a LLM like this:\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI  # or ChatOllama\n",
    "\n",
    "llm = ChatOpenAI()  # or use ChatOllama(model=\"gemma:2b\")\n",
    "\n",
    "# Use top result as context\n",
    "context = sorted_results[0][0]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"context\": context, \"question\": query})\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "| Step | Action                                 | Tool                             |\n",
    "| ---- | -------------------------------------- | -------------------------------- |\n",
    "| 1ï¸âƒ£  | Convert your documents to embeddings   | `OllamaEmbeddings`               |\n",
    "| 2ï¸âƒ£  | Convert your query to an embedding     | `OllamaEmbeddings.embed_query()` |\n",
    "| 3ï¸âƒ£  | Compare vectors to find most similar   | `cosine_similarity()`            |\n",
    "| 4ï¸âƒ£  | Feed relevant docs to LLM for response | `langchain` prompt + LLM         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8338acd9-5832-40bb-947c-60f4be3bc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_21648\\3021116706.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hembedding = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(model='gemma:2b')\n",
    "\n",
    "r = embedding.embed_query('')\n",
    "print(len(r))\n",
    "print('')\n",
    "#from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "hembedding = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "r = hembedding.embed_query('')\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e129979f-541a-46a5-9b64-c190fb503cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='8d04df27-8621-4d27-9b75-0faf9e0be615', metadata={'source': 'my_file.txt'}, page_content='Langchain is a powerful framework for building applications with large language models.\\ncore components of langchains chains,agent,memory,retrievers and other tools.\\nsample RAG applications.\\nGuido vanrosum is an author of python programming.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "loader = TextLoader('my_file.txt')\n",
    "documents = loader.load()\n",
    "## step-1 \n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500,chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "## step-2\n",
    "\n",
    "#embeddings = OllamaEmbeddings(model='gemma:2b')\n",
    "## step-3\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "db = FAISS.from_documents(docs,embeddings)\n",
    "## step-4\n",
    "r = db.similarity_search('What is langchain?')\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "923b57a0-0d0a-4a87-8801-41323e190043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='1e0e839b-a206-462a-a2e0-8b6b0a8cda27', metadata={'source': 'my_file.txt'}, page_content='Langchain is a powerful framework for building applications with large language models.\\ncore components of langchains chains,agent,memory,retrievers and other tools.\\nsample RAG applications.\\nGuido vanrosum is an author of python programming.')]\n",
      "\n",
      "Answer: I do not have access to external sources or the context of the question, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA #\n",
    "from langchain_community.llms import Ollama #\n",
    "loader = TextLoader('my_file.txt')\n",
    "documents = loader.load()\n",
    "## step-1 \n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500,chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "## step-2\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='gemma:2b')\n",
    "## step-3\n",
    "\n",
    "db = FAISS.from_documents(docs,embeddings)\n",
    "## step-4\n",
    "r = db.similarity_search('What is langchain?')\n",
    "print(r)\n",
    "\n",
    "# 5. Setup retriever\n",
    "retriever = db.as_retriever()\n",
    "# 6. Use Ollama model\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "# 7. Build RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,retriever=retriever,return_source_documents=True)\n",
    "\n",
    "# 8. Ask question\n",
    "query=\"What is Langchain and how does RAG work?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "# 9.Output the result\n",
    "print(\"\\nAnswer:\",result['result'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2730068e-e90d-4bf4-958a-79ab08a16f05",
   "metadata": {},
   "source": [
    "PromptTemplate, which is a core concept when working with LLMs\n",
    "\n",
    "What Is a PromptTemplate?\n",
    "===========================\n",
    "A PromptTemplate is a customizable string that you feed to a language model (LLM). \n",
    "It defines how you ask your question and how you format any context or variables \n",
    "you want to pass to the model.\n",
    "\n",
    "In LangChain, PromptTemplate is used to:\n",
    "==========================================\n",
    "Define the structure of your prompts\n",
    "\n",
    "Insert dynamic variables like a query, context, examples, etc.\n",
    "\n",
    "Guide the model to respond in a specific format or tone\n",
    "\n",
    "Example of a Basic PromptTemplate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "In this example:\n",
    "\n",
    "{context} and {question} are placeholders\n",
    "\n",
    "You fill in those fields dynamically when you call .format() or run a chain\n",
    "\n",
    "Why Use PromptTemplates?\n",
    "Using a PromptTemplate allows you to:\n",
    "\n",
    "Feature\tBenefit\n",
    "ðŸŽ›ï¸ Parameterize prompts\tReuse the same structure with different input\n",
    "ðŸ§  Guide LLM behavior\tGive clear instructions and avoid hallucination\n",
    "ðŸ” Chain-friendly\tEasily integrated with LangChain chains like RetrievalQA or LLMChain\n",
    "\n",
    "ðŸ§ª How It Works Internally\n",
    "LangChain templates use standard Python string formatting under the hood.\n",
    "\n",
    "For example:\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Answer the following question based only on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "formatted_prompt = template.format(\n",
    "    context=\"Apples are fruits that are usually red or green.\",\n",
    "    question=\"What color are apples?\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)\n",
    "ðŸ“¦ Output:\n",
    "\n",
    "Answer the following question based only on the context below.\n",
    "\n",
    "Context:\n",
    "Apples are fruits that are usually red or green.\n",
    "\n",
    "Question:\n",
    "What color are apples?\n",
    "\n",
    "Answer:\n",
    "ðŸ”„ PromptTemplate in a RetrievalQA Setup\n",
    "Hereâ€™s how you integrate it with a Retrieval-Augmented Generation (RAG) pipeline:\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant. Use ONLY the context below to answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "Here, LangChain will:\n",
    "\n",
    "Retrieve the top documents from FAISS\n",
    "\n",
    "Format the prompt using the template above\n",
    "\n",
    "Feed the fully constructed prompt to the LLM\n",
    "\n",
    "Return the generated answer\n",
    "\n",
    "ðŸ“š Tips for Writing Good Prompts\n",
    "Tip\tWhy\n",
    "âœ… Be explicit\tSmall models often need detailed instructions\n",
    "âœ… Include example formats\tGuide the model's output shape\n",
    "âœ… Instruct it NOT to guess\tPrevent hallucinations when context is missing\n",
    "âœ… Keep it simple and clear\tAvoid confusing or compound instructions\n",
    "\n",
    "ðŸ” Common PromptTemplate Use Cases\n",
    "Use Case\tPrompt Purpose\n",
    "RetrievalQA\tUse retrieved context to answer\n",
    "LLMChain\tUse LLM for summarizing, rewriting, or answering\n",
    "Few-shot\tInclude examples in the prompt\n",
    "ChatPromptTemplate\tFor multi-turn conversations (LangChain supports this too)\n",
    "\n",
    "ðŸ§  Summary\n",
    "PromptTemplate lets you:\n",
    "Insert dynamic values (like context and question)\n",
    "\n",
    "Control the format of prompts to the LLM\n",
    "\n",
    "Avoid hallucination or ambiguity\n",
    "\n",
    "Improve answer quality â€” especially with open-source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7373ac67-c594-4982-8f12-3ad75ccd408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Sure, here is the answer:\n",
      "\n",
      "**LangChain** is a framework for developing applications powered by language models. It provides tools for chaining LLMs with sources of data like documents or APIs.\n",
      "\n",
      "**RAG** (Retrieval-Augmented Generation) is a method that combines external knowledge retrieval with generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Documents (make sure they mention LangChain + RAG)\n",
    "documents = [Document(page_content=\"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It provides tools for chaining LLMs with sources of data like documents or APIs.\n",
    "RAG (Retrieval-Augmented Generation) combines external knowledge retrieval with generation.\n",
    "\"\"\")]\n",
    "\n",
    "# 2. Text split\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# 3. Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Vector DB\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# 5. Prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question. Be factual and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# 6. LLM\n",
    "llm = Ollama(model=\"gemma:2b\")  # or gemma:2b if you want\n",
    "\n",
    "# 7. RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 8. Ask a question\n",
    "query = \"What is LangChain and how does RAG work?\"\n",
    "result = qa(query)\n",
    "\n",
    "# 9. Output\n",
    "print(\"Answer:\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541abe3-e850-4d20-9aef-c5ff19840c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
