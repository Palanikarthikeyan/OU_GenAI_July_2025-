{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6a1722-1219-4c2f-b290-b9da580fc7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308db03b-5a97-4101-8125-227a47362158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_13528\\840512378.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5e7e16-e30f-421f-9879-4ba67e17e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "gK = os.getenv('GROQ_API_KEY')\n",
    "llm = ChatGroq(model='Llama3-8b-8192',groq_api_key=gK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7df1476-7d03-43a2-9985-fb3262212dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b61bfa-2c20-4047-9fa4-f35934ab6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a4ded2-2fc6-480a-ade8-d4d05eed9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "url='https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "loader = WebBaseLoader(web_path=(url),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\")),))\n",
    "docs = loader.load()\n",
    "print(type(docs),len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b114e11-5d4e-43dc-a19e-f5fe16c25d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab32846d-a05f-4415-8c33-1a4af5e7b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe17733-ced5-4b65-b727-314719577217",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits,embedding=embeddings)\n",
    "r = vectorstore.similarity_search('What is langchain?')\n",
    "#pprint.pprint(r) # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7163a64d-5945-4e4c-8619-91854a974563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Reflection is a vital aspect of autonomous agents, allowing them to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "\n",
      "In the context of the ReAct framework, Self-Reflection is the process of analyzing and learning from past actions and their outcomes. This involves reflecting on the thought process, actions taken, and observations made during a task or episode.\n",
      "\n",
      "The ReAct framework uses a prompt template to guide this reflection, which includes explicit steps for the agent to think, act, and observe. The template is formatted as:\n",
      "\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "\n",
      "This process allows the agent to identify inefficiencies in its planning and correct hallucinations (sequences of identical actions that lead to the same observation). The agent's working memory stores up to three reflections, which are used as context for querying the LLM (Large Language Model).\n",
      "\n",
      "Self-Reflection is essential for autonomous agents to learn from their mistakes, adapt to new situations, and improve their performance over time. It enables them to refine their decision-making processes, adjust their strategies, and optimize their actions for better outcomes.\n"
     ]
    }
   ],
   "source": [
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\"you are AI assistant\" \"\\n\\n{context}\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\"{input}\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# connect llm\n",
    "#llm_obj = Ollama(model='gemma:2b')\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm,prompt)\n",
    "ret_chain = create_retrieval_chain(retriver,qa_chain)\n",
    "\n",
    "response = ret_chain.invoke({'input':'what is Self-Reflection'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb147081-3d9c-4224-89e8-0742f36846c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of human brains, memory can be categorized into several types based on its characteristics, duration, and functionality. Here's a breakdown of the main types of memory:\n",
      "\n",
      "1. **Sensory Memory**: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, tactile, etc.) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds.\n",
      "\n",
      "Subcategories of sensory memory include:\n",
      "\n",
      "* **Iconic Memory** (visual): temporary storage of visual information, such as images or patterns.\n",
      "* **Echoic Memory** (auditory): temporary storage of auditory information, such as sounds or melodies.\n",
      "* **Haptic Memory** (touch): temporary storage of tactile information, such as touch or vibrations.\n",
      "\n",
      "2. **Short-Term Memory (STM) or Working Memory**: This type of memory stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have a limited capacity (about 7 Â± 2 items, according to Miller's Law) and lasts for 20-30 seconds.\n",
      "\n",
      "3. **Long-Term Memory (LTM)**: Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. LTM is divided into two subtypes:\n",
      "\n",
      "* **Explicit / Declarative Memory**: This type of memory involves memory of facts and events, and refers to those memories that can be consciously recalled. Examples include:\n",
      "\t+ **Episodic Memory**: memory of specific events and experiences.\n",
      "\t+ **Semantic Memory**: memory of facts and concepts.\n",
      "* **Implicit / Procedural Memory**: This type of memory is unconscious and involves skills and routines that are performed automatically, such as:\n",
      "\t+ **Motor Skills**: skills that are performed through physical movement, like riding a bike or playing a musical instrument.\n",
      "\t+ **Cognitive Skills**: skills that are performed through mental processes, like typing on a keyboard or solving a puzzle.\n",
      "\n",
      "These categories are not mutually exclusive, and there can be some overlap between them. However, this breakdown provides a general framework for understanding the different types of memory and how they function.\n"
     ]
    }
   ],
   "source": [
    "response = ret_chain.invoke({'input':'explain types of memory'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5545d13b-c70c-47f7-99a2-fcc11c8f7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint.pprint(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dca63f6-752d-444e-8c96-72e2ad25ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDDL stands for Planning Domain Definition Language. It is a formal language used to describe planning problems and domain knowledge. It was developed specifically for planning, which is the process of finding a sequence of actions to achieve a specific goal.\n",
      "\n",
      "PDDL provides a common language for describing planning problems, making it possible for different planning systems and tools to communicate with each other. It is widely used in the field of artificial intelligence (AI) and robotics, particularly in areas such as:\n",
      "\n",
      "1. Planning: PDDL is used to describe planning problems, including the initial state, goals, and constraints.\n",
      "2. Robotics: PDDL is used to describe the planning problems for robots, including the robot's capabilities, the environment, and the goals.\n",
      "3. Artificial Intelligence: PDDL is used in AI research to describe planning problems and test planning algorithms.\n",
      "\n",
      "PDDL has two main components:\n",
      "\n",
      "1. Domain Description: This component describes the planning domain, including the objects, actions, and constraints.\n",
      "2. Problem Description: This component describes the planning problem, including the initial state, goals, and constraints.\n",
      "\n",
      "PDDL provides a way to formally describe planning problems, making it possible to:\n",
      "\n",
      "1. Automate planning: PDDL allows planning systems to automatically generate plans to achieve the goals.\n",
      "2. Compare planning systems: PDDL provides a common language for comparing the performance of different planning systems.\n",
      "3. Share planning knowledge: PDDL enables the sharing of planning knowledge and domain descriptions across different planning systems and domains.\n",
      "\n",
      "Overall, PDDL is a powerful tool for representing and solving planning problems, and it has many applications in AI, robotics, and other fields.\n"
     ]
    }
   ],
   "source": [
    "response = ret_chain.invoke({'input':'what is pddl?'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07a92878-46ca-496e-97e2-86d5a3e4d594",
   "metadata": {},
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\"you are AI assistant\" \"\\n\\n say don't answer\\n\\n{context}\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\"{input}\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6f4d5-7fc8-4471-b6b3-6518fbf8aa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
