{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0601f059-244b-4b7e-9fdd-3ae48a215e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def f1(fname): # load json to documents in llm - dataloading\n",
    "    with open(fname) as fobj:\n",
    "        data = json.load(fobj)\n",
    "\n",
    "    docs = []\n",
    "    for var in data:\n",
    "        content = var.get(\"content\",\" \")\n",
    "        metadata_result = {k: v for k,v, in var.items() if k != 'content'}\n",
    "        docs.append(Document(page_content=content,metadata=metadata_result))\n",
    "    return docs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c526f90-f3f6-43fe-a0ca-a7ea25a5b505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'LangChain Overview'}, page_content='LangChain is a framework for developing applications powered by language models.'),\n",
       " Document(metadata={'title': 'What is Ollama?'}, page_content='Ollama allows running large language models locally like LLaMA and Mistral.'),\n",
       " Document(metadata={'title': 'Embeddings in NLP'}, page_content='Embeddings are vector representations of text used for similarity and retrieval.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = f1('data.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac9febd-5f24-4bd9-a81c-6368694ebebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76a4903-5fcc-40b9-8557-4a633e86b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_10316\\933803318.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model=\"gemma:2b\")\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=50)\n",
    "splitted_docs = splitter.split_documents(data)\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "\n",
    "vectordb = Chroma.from_documents(splitted_docs,embedding=embedding)\n",
    "retriver = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c88deba-a75c-4321-886d-8d50b3b9b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6543ffca-6a82-45a4-816e-5ec6a595f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_10316\\3309139237.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm_obj = Ollama(model='gemma:2b')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is langchain?', 'context': [Document(metadata={'title': 'LangChain Overview'}, page_content='LangChain is a framework for developing applications powered by language models.'), Document(metadata={'title': 'What is Ollama?'}, page_content='Ollama allows running large language models locally like LLaMA and Mistral.'), Document(metadata={'title': 'Embeddings in NLP'}, page_content='Embeddings are vector representations of text used for similarity and retrieval.')], 'answer': \"Sure, here's the answer to your question:\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLM's). It offers a robust platform for building, training, and deploying robust language-based solutions.\\n\\nLangChain provides various tools and features that facilitate the development of diverse applications. These include:\\n\\n* **Ollama:** An API-based library for running LLaMA and Mistral models locally.\\n* **Embeddings:** Vector representations of text for similarity and retrieval.\\n* **Data Management:** A comprehensive data pipeline for loading, cleaning, and transforming data.\\n* **Application Programming Interface (API):** A well-defined API that allows developers to integrate LLM solutions seamlessly into existing workflows.\\n* **Community Resources:** A vibrant community of developers and users, providing support, resources, and collaboration opportunities.\\n\\nLangChain is particularly well-suited for building complex and scalable language-based solutions across various domains, including:\\n\\n* **Chatbots:** Create interactive and engaging chatbots with natural language processing capabilities.\\n* **Language Modeling:** Train and optimize LLM's for various tasks, such as text generation, translation, and question answering.\\n* **Text Analysis:** Perform sentiment analysis, topic modeling, and other natural language processing tasks.\\n* **Virtual Assistants:** Develop intelligent virtual assistants that can assist users with various tasks.\\n* **Content Creation:** Generate high-quality text, code, and other creative content.\\n\\nOverall, LangChain provides a comprehensive and efficient framework for building and implementing advanced language-based applications.\"}\n"
     ]
    }
   ],
   "source": [
    "# define your prompt\n",
    "prompt_obj = PromptTemplate.from_template('''You are an expert assistant {context} Question:{input} Answer:''')\n",
    "# connect llm\n",
    "llm_obj = Ollama(model='gemma:2b')\n",
    "\n",
    "# create stuff chain to combine retrived docs\n",
    "doc_chain = create_stuff_documents_chain(llm=llm_obj,prompt=prompt_obj)\n",
    "# create retrieved chain\n",
    "rag_chain = create_retrieval_chain(retriever=retriver,combine_docs_chain=doc_chain)\n",
    "\n",
    "# invoke query\n",
    "response = rag_chain.invoke({'input':'what is langchain?'})\n",
    "# display response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e157c-3ec3-4746-b0b8-322a80a0765c",
   "metadata": {},
   "source": [
    "import pprint\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f703e201-72ed-4af3-ae3c-ea0dcb1b7394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Sure, here's the answer to your question:\\n\"\n",
      " '\\n'\n",
      " '**LangChain** is a framework for developing applications powered by large '\n",
      " \"language models (LLM's). It offers a robust platform for building, training, \"\n",
      " 'and deploying robust language-based solutions.\\n'\n",
      " '\\n'\n",
      " 'LangChain provides various tools and features that facilitate the '\n",
      " 'development of diverse applications. These include:\\n'\n",
      " '\\n'\n",
      " '* **Ollama:** An API-based library for running LLaMA and Mistral models '\n",
      " 'locally.\\n'\n",
      " '* **Embeddings:** Vector representations of text for similarity and '\n",
      " 'retrieval.\\n'\n",
      " '* **Data Management:** A comprehensive data pipeline for loading, cleaning, '\n",
      " 'and transforming data.\\n'\n",
      " '* **Application Programming Interface (API):** A well-defined API that '\n",
      " 'allows developers to integrate LLM solutions seamlessly into existing '\n",
      " 'workflows.\\n'\n",
      " '* **Community Resources:** A vibrant community of developers and users, '\n",
      " 'providing support, resources, and collaboration opportunities.\\n'\n",
      " '\\n'\n",
      " 'LangChain is particularly well-suited for building complex and scalable '\n",
      " 'language-based solutions across various domains, including:\\n'\n",
      " '\\n'\n",
      " '* **Chatbots:** Create interactive and engaging chatbots with natural '\n",
      " 'language processing capabilities.\\n'\n",
      " \"* **Language Modeling:** Train and optimize LLM's for various tasks, such as \"\n",
      " 'text generation, translation, and question answering.\\n'\n",
      " '* **Text Analysis:** Perform sentiment analysis, topic modeling, and other '\n",
      " 'natural language processing tasks.\\n'\n",
      " '* **Virtual Assistants:** Develop intelligent virtual assistants that can '\n",
      " 'assist users with various tasks.\\n'\n",
      " '* **Content Creation:** Generate high-quality text, code, and other creative '\n",
      " 'content.\\n'\n",
      " '\\n'\n",
      " 'Overall, LangChain provides a comprehensive and efficient framework for '\n",
      " 'building and implementing advanced language-based applications.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b227176b-1413-40c2-bdd6-5b3959717b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is langchain?', 'context': [Document(metadata={'title': 'LangChain Overview'}, page_content='LangChain is a framework for developing applications powered by language models.'), Document(metadata={'title': 'What is Ollama?'}, page_content='Ollama allows running large language models locally like LLaMA and Mistral.'), Document(metadata={'title': 'Embeddings in NLP'}, page_content='Embeddings are vector representations of text used for similarity and retrieval.')], 'answer': 'The context does not provide any information about what langchain is, so I cannot answer this question from the provided context.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([('system','you are exper assistant. use the below context to answer the question'),\n",
    "                                  ('human','content:\\n{context}\\n\\nQuestion: {input}')])\n",
    "\n",
    "llmobj = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(llm=llmobj,prompt=chat_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever=retriver,combine_docs_chain=doc_chain)\n",
    "\n",
    "# invoke query\n",
    "response = rag_chain.invoke({'input':'what is langchain?'})\n",
    "# display response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a3101cb-f1e7-4b9b-a8ca-c2cae9485325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "# Use webase loader \n",
    "# https://lilianweng.github.io/posts/2023-06-23-agent/\n",
    "# -----------------------------------------------------\n",
    "# data loading - webaseload ->bs4.SoupStrainer(class_=(\"post-title\",\"post-content\",\"post-header\"))\n",
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "import bs4\n",
    "\n",
    "url='https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "loader = WebBaseLoader(web_path=(url),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-title\",\"post-content\",\"post-header\")),))\n",
    "docs = loader.load()\n",
    "print(type(docs),len(docs))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7158c7b6-dc7c-4ba8-8593-27f3f1ef58b9",
   "metadata": {},
   "source": [
    "# Use RecursiveCharacterTextSplitter ->split text docs\n",
    "#\n",
    "# create db ->stores to DB # Use Chroma \n",
    "# \n",
    "# Get vector retriverver => vectorstore.as_retriever()\n",
    "#\n",
    "# PromptTemplate - ChatPromptTemplate\n",
    "#\n",
    "# create_stuff_documents_chain(llm,prompt)\n",
    "# \n",
    "# create_retrieval_chain(....)\n",
    "#\n",
    "#using retrieval_object-invoke a method\n",
    "#                         |--->Refer website: what is ...\n",
    "# display response \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6118b7d9-7c8e-4f24-8241-1fa066e882f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53160e0b-3abf-4e27-bcf2-eb353859f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2ab495-1ab6-49bb-96e7-e3cb7321e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "url='https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "loader = WebBaseLoader(web_path=(url),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\")),))\n",
    "docs = loader.load()\n",
    "print(type(docs),len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f545da2a-2a09-4101-a26e-c91929e2afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a6708a6-6d73-4748-b74e-e4e9c9556335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#InvalidArgumentError: Collection expecting embedding with dimension of 2048, got 384\n",
    "\n",
    "# embeddings = OllamaEmbeddings(model=\"gemma:2b\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5841f88f-f5c9-4cd1-9030-24df185a4cc6",
   "metadata": {},
   "source": [
    "# vectorstore = Chroma.from_documents(documents=splits,embedding=embeddings)\n",
    "r = vectorstore.similarity_search('What is langchain?')\n",
    "pprint.pprint(r) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3121cb4-1dbf-43b7-9a15-fc0673a502bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b8293d2-8053-41d8-a7b9-14ce48e1ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_10316\\796224928.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e813cbc5-d2b0-4fb0-bde6-62dc95d1c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma.from_documents(documents=splits,embedding=embeddings)\n",
    "# InvalidArgumentError: Collection expecting embedding with dimension of 2048, got 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d47b51-cfa5-4a68-b90a-15df2ecbe7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([('system','you are AI assist\\n{context} Use \n",
    "                                             the context to answer the question'),\n",
    "                                 ('human','{input}')\n",
    "                                 ])\n",
    "\n",
    "# connect llm\n",
    "llm_obj = Ollama(model='gemma:2b')\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm_obj,prompt)\n",
    "ret_chain = create_retrieval_chain(retriver,qa_chain)\n",
    "\n",
    "response = ret_chain.invoke({'input':'what is Self-Reflection'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c025c896-2076-4a0d-b12b-b18012e20a95",
   "metadata": {},
   "source": [
    "response = ret_chain.invoke({'input':'....'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d890c-0803-4660-a800-03c2d7b947f5",
   "metadata": {},
   "source": [
    "## Refer this OU-GEN-LLM-4-Activity-WebbaseLoader.ipynb Jupyter file\n",
    "## \n",
    "https://github.com/Palanikarthikeyan/OU_GenAI_July_2025-/blob/main/DAY4/OU-GEN-LLM-4-Activity-WebbaseLoader.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
